{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "os.environ['OMP_NUM_THREADS'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imagename</th>\n",
       "      <th>postid</th>\n",
       "      <th>dataset</th>\n",
       "      <th>dataset.1</th>\n",
       "      <th>postdate</th>\n",
       "      <th>Contents</th>\n",
       "      <th>url</th>\n",
       "      <th>Q1_pertain_to_covid</th>\n",
       "      <th>Q2_cetegory</th>\n",
       "      <th>Q2A_Type of Human</th>\n",
       "      <th>...</th>\n",
       "      <th>Q8_threat_covid</th>\n",
       "      <th>Q9_susceptibility_covid</th>\n",
       "      <th>Q9A_Asian responsible for the covid</th>\n",
       "      <th>Q10_solution_present</th>\n",
       "      <th>Q11_recommended_solution</th>\n",
       "      <th>misinformation</th>\n",
       "      <th>Q12 Presence of conspiracy theory</th>\n",
       "      <th>Q12-Others</th>\n",
       "      <th>Q13.  Image of plague doctor costume</th>\n",
       "      <th>Note</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B-A5FIIIEKJ</td>\n",
       "      <td>5592</td>\n",
       "      <td>middle</td>\n",
       "      <td>2</td>\n",
       "      <td>3/21/20</td>\n",
       "      <td>#covid #covid2020 #covidvirus #virus #coronava...</td>\n",
       "      <td>https://www.instagram.com/p/B-A5FIIIEKJ/</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>99</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Stay home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B-A74YQn_4a</td>\n",
       "      <td>5593</td>\n",
       "      <td>middle</td>\n",
       "      <td>2</td>\n",
       "      <td>3/21/20</td>\n",
       "      <td>Well this is the final mural of my trip in Aus...</td>\n",
       "      <td>https://www.instagram.com/p/B-A74YQn_4a/</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>99</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>99</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B-AajnDp6GQ</td>\n",
       "      <td>5575</td>\n",
       "      <td>middle</td>\n",
       "      <td>2</td>\n",
       "      <td>3/21/20</td>\n",
       "      <td>Chegamos !!! V√£o seguindo o movimento... Tem m...</td>\n",
       "      <td>https://www.instagram.com/p/B-AajnDp6GQ/</td>\n",
       "      <td>2</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>...</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>0</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B-AarR4pUQA</td>\n",
       "      <td>5576</td>\n",
       "      <td>middle</td>\n",
       "      <td>2</td>\n",
       "      <td>3/21/20</td>\n",
       "      <td>üòªüòªüòªüòªüòª</td>\n",
       "      <td>https://www.instagram.com/p/B-AarR4pUQA/</td>\n",
       "      <td>2</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>...</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>0</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B-AE1y3HD-Z</td>\n",
       "      <td>5550</td>\n",
       "      <td>middle</td>\n",
       "      <td>2</td>\n",
       "      <td>3/21/20</td>\n",
       "      <td>EN MI DOMICILIO üè°\\r\\r\\r\\r\\n#quedateencasa\\r\\r\\...</td>\n",
       "      <td>https://www.instagram.com/p/B-AE1y3HD-Z/</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>99</td>\n",
       "      <td>2.0</td>\n",
       "      <td>mask; Stay home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9643</th>\n",
       "      <td>CEZyheBgtMd</td>\n",
       "      <td>8402</td>\n",
       "      <td>second</td>\n",
       "      <td>3</td>\n",
       "      <td>8/27/20</td>\n",
       "      <td>#water #foryou #followforfollowback #photograp...</td>\n",
       "      <td>https://www.instagram.com/p/CEZyheBgtMd/</td>\n",
       "      <td>2</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>...</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>0</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9644</th>\n",
       "      <td>CEZYoyRluKG</td>\n",
       "      <td>8352</td>\n",
       "      <td>second</td>\n",
       "      <td>3</td>\n",
       "      <td>8/27/20</td>\n",
       "      <td>#like4likes #20likes #tagforlikes #instalikes ...</td>\n",
       "      <td>https://www.instagram.com/p/CEZYoyRluKG/</td>\n",
       "      <td>2</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>...</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>0</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9645</th>\n",
       "      <td>CEZz-solaUF</td>\n",
       "      <td>8403</td>\n",
       "      <td>second</td>\n",
       "      <td>3</td>\n",
       "      <td>8/27/20</td>\n",
       "      <td>üôàü•∞üòçüëâüèΩ @love_serie_karma #daancorona #daancoron...</td>\n",
       "      <td>https://www.instagram.com/p/CEZz-solaUF/</td>\n",
       "      <td>2</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>...</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>0</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9646</th>\n",
       "      <td>CEZZ4P4j3rh</td>\n",
       "      <td>8354</td>\n",
       "      <td>second</td>\n",
       "      <td>3</td>\n",
       "      <td>8/27/20</td>\n",
       "      <td>üí•üö® ùóôùóîùóüùóü ùóúùó¶ ùóñùó¢ùó†ùóúùó°ùóö üö®üí•‚Å£\\r\\r\\r\\r\\r\\n‚Å£\\r\\r\\r\\r\\r\\n...</td>\n",
       "      <td>https://www.instagram.com/p/CEZZ4P4j3rh/</td>\n",
       "      <td>2</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>...</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>0</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9647</th>\n",
       "      <td>CEZZyqshFLq</td>\n",
       "      <td>8353</td>\n",
       "      <td>second</td>\n",
       "      <td>3</td>\n",
       "      <td>8/27/20</td>\n",
       "      <td>Follow me @kestine_kylie \\r\\r\\r\\r\\n.\\r\\r\\r\\r\\n...</td>\n",
       "      <td>https://www.instagram.com/p/CEZZyqshFLq/</td>\n",
       "      <td>2</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>...</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>0</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9648 rows √ó 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        imagename  postid dataset  dataset.1 postdate  \\\n",
       "0     B-A5FIIIEKJ    5592  middle          2  3/21/20   \n",
       "1     B-A74YQn_4a    5593  middle          2  3/21/20   \n",
       "2     B-AajnDp6GQ    5575  middle          2  3/21/20   \n",
       "3     B-AarR4pUQA    5576  middle          2  3/21/20   \n",
       "4     B-AE1y3HD-Z    5550  middle          2  3/21/20   \n",
       "...           ...     ...     ...        ...      ...   \n",
       "9643  CEZyheBgtMd    8402  second          3  8/27/20   \n",
       "9644  CEZYoyRluKG    8352  second          3  8/27/20   \n",
       "9645  CEZz-solaUF    8403  second          3  8/27/20   \n",
       "9646  CEZZ4P4j3rh    8354  second          3  8/27/20   \n",
       "9647  CEZZyqshFLq    8353  second          3  8/27/20   \n",
       "\n",
       "                                               Contents  \\\n",
       "0     #covid #covid2020 #covidvirus #virus #coronava...   \n",
       "1     Well this is the final mural of my trip in Aus...   \n",
       "2     Chegamos !!! V√£o seguindo o movimento... Tem m...   \n",
       "3                                                 üòªüòªüòªüòªüòª   \n",
       "4     EN MI DOMICILIO üè°\\r\\r\\r\\r\\n#quedateencasa\\r\\r\\...   \n",
       "...                                                 ...   \n",
       "9643  #water #foryou #followforfollowback #photograp...   \n",
       "9644  #like4likes #20likes #tagforlikes #instalikes ...   \n",
       "9645  üôàü•∞üòçüëâüèΩ @love_serie_karma #daancorona #daancoron...   \n",
       "9646  üí•üö® ùóôùóîùóüùóü ùóúùó¶ ùóñùó¢ùó†ùóúùó°ùóö üö®üí•‚Å£\\r\\r\\r\\r\\r\\n‚Å£\\r\\r\\r\\r\\r\\n...   \n",
       "9647  Follow me @kestine_kylie \\r\\r\\r\\r\\n.\\r\\r\\r\\r\\n...   \n",
       "\n",
       "                                           url  Q1_pertain_to_covid  \\\n",
       "0     https://www.instagram.com/p/B-A5FIIIEKJ/                    1   \n",
       "1     https://www.instagram.com/p/B-A74YQn_4a/                    1   \n",
       "2     https://www.instagram.com/p/B-AajnDp6GQ/                    2   \n",
       "3     https://www.instagram.com/p/B-AarR4pUQA/                    2   \n",
       "4     https://www.instagram.com/p/B-AE1y3HD-Z/                    1   \n",
       "...                                        ...                  ...   \n",
       "9643  https://www.instagram.com/p/CEZyheBgtMd/                    2   \n",
       "9644  https://www.instagram.com/p/CEZYoyRluKG/                    2   \n",
       "9645  https://www.instagram.com/p/CEZz-solaUF/                    2   \n",
       "9646  https://www.instagram.com/p/CEZZ4P4j3rh/                    2   \n",
       "9647  https://www.instagram.com/p/CEZZyqshFLq/                    2   \n",
       "\n",
       "      Q2_cetegory  Q2A_Type of Human  ...  Q8_threat_covid  \\\n",
       "0               1                  1  ...                1   \n",
       "1               1                  1  ...                1   \n",
       "2              99                 99  ...               99   \n",
       "3              99                 99  ...               99   \n",
       "4               1                  1  ...                1   \n",
       "...           ...                ...  ...              ...   \n",
       "9643           99                 99  ...               99   \n",
       "9644           99                 99  ...               99   \n",
       "9645           99                 99  ...               99   \n",
       "9646           99                 99  ...               99   \n",
       "9647           99                 99  ...               99   \n",
       "\n",
       "      Q9_susceptibility_covid  Q9A_Asian responsible for the covid  \\\n",
       "0                           1                                    2   \n",
       "1                           1                                    2   \n",
       "2                          99                                   99   \n",
       "3                          99                                   99   \n",
       "4                           1                                    2   \n",
       "...                       ...                                  ...   \n",
       "9643                       99                                   99   \n",
       "9644                       99                                   99   \n",
       "9645                       99                                   99   \n",
       "9646                       99                                   99   \n",
       "9647                       99                                   99   \n",
       "\n",
       "      Q10_solution_present  Q11_recommended_solution  misinformation  \\\n",
       "0                        1                         1               0   \n",
       "1                        2                        99               0   \n",
       "2                       99                        99               0   \n",
       "3                       99                        99               0   \n",
       "4                        1                         1               0   \n",
       "...                    ...                       ...             ...   \n",
       "9643                    99                        99               0   \n",
       "9644                    99                        99               0   \n",
       "9645                    99                        99               0   \n",
       "9646                    99                        99               0   \n",
       "9647                    99                        99               0   \n",
       "\n",
       "      Q12 Presence of conspiracy theory  Q12-Others  \\\n",
       "0                                     2          99   \n",
       "1                                     2          99   \n",
       "2                                    99          99   \n",
       "3                                    99          99   \n",
       "4                                     2          99   \n",
       "...                                 ...         ...   \n",
       "9643                                 99          99   \n",
       "9644                                 99          99   \n",
       "9645                                 99          99   \n",
       "9646                                 99          99   \n",
       "9647                                 99          99   \n",
       "\n",
       "      Q13.  Image of plague doctor costume             Note  \n",
       "0                                      2.0        Stay home  \n",
       "1                                      2.0              NaN  \n",
       "2                                     99.0              NaN  \n",
       "3                                     99.0              NaN  \n",
       "4                                      2.0  mask; Stay home  \n",
       "...                                    ...              ...  \n",
       "9643                                  99.0              NaN  \n",
       "9644                                  99.0              NaN  \n",
       "9645                                  99.0              NaN  \n",
       "9646                                  99.0              NaN  \n",
       "9647                                  99.0              NaN  \n",
       "\n",
       "[9648 rows x 26 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "ins_df = pd.read_csv('data/instagram_data.csv')\n",
    "ins_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>@RossKemp great programme tonight #sad #upsett...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>Yo this 3 v 3 overtime in hockey got me sweaty...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>#Trump √¢‚Ç¨‚Ñ¢s √¢‚Ç¨ÀúMake America Great Again√¢‚Ç¨‚Ñ¢ pla...</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>having a pet store worker ask 'do you want to ...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>@YahooCare data stolen in 2014 and only now do...</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>@liamch88 yeah! :) playing well</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>676</th>\n",
       "      <td>Do not be discouraged by a slowing sales marke...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>Watch this amazing live.ly broadcast by @its.f...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>@khloe_speaks sad music</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>it was both lively &amp;amp; lovely @crumblepie15 ...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3613 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 tweet category\n",
       "198  @RossKemp great programme tonight #sad #upsett...  sadness\n",
       "158  Yo this 3 v 3 overtime in hockey got me sweaty...      joy\n",
       "503  #Trump √¢‚Ç¨‚Ñ¢s √¢‚Ç¨ÀúMake America Great Again√¢‚Ç¨‚Ñ¢ pla...     fear\n",
       "91   having a pet store worker ask 'do you want to ...      joy\n",
       "112  @YahooCare data stolen in 2014 and only now do...     fear\n",
       "..                                                 ...      ...\n",
       "153                   @liamch88 yeah! :) playing well       joy\n",
       "676  Do not be discouraged by a slowing sales marke...  sadness\n",
       "326  Watch this amazing live.ly broadcast by @its.f...      joy\n",
       "296                            @khloe_speaks sad music  sadness\n",
       "411  it was both lively &amp; lovely @crumblepie15 ...      joy\n",
       "\n",
       "[3613 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "anger_df = pd.read_csv('data/twitter/anger.tsv', sep='\\t').drop(columns=['index', 'intensity'])\n",
    "fear_df = pd.read_csv('data/twitter/fear.tsv', sep='\\t').drop(columns=['index', 'intensity'])\n",
    "joy_df = pd.read_csv('data/twitter/joy.tsv', sep='\\t').drop(columns=['index', 'intensity'])\n",
    "sadness_df = pd.read_csv('data/twitter/sadness.tsv', sep='\\t').drop(columns=['index', 'intensity'])\n",
    "\n",
    "emotion_df = pd.concat([anger_df, fear_df, joy_df, sadness_df])\n",
    "emotion_df = shuffle(emotion_df)\n",
    "\n",
    "emotion_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: transformers in c:\\users\\hp\\anaconda3\\lib\\site-packages (4.12.5)\n",
      "Requirement already up-to-date: datasets in c:\\users\\hp\\anaconda3\\lib\\site-packages (1.15.1)\n",
      "Requirement already satisfied, skipping upgrade: huggingface-hub<1.0,>=0.1.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (0.1.2)\n",
      "Requirement already satisfied, skipping upgrade: requests in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (2.26.0)\n",
      "Requirement already satisfied, skipping upgrade: regex!=2019.12.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (2020.10.15)\n",
      "Requirement already satisfied, skipping upgrade: packaging>=20.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (20.4)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (1.21.4)\n",
      "Requirement already satisfied, skipping upgrade: tqdm>=4.27 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (4.50.2)\n",
      "Requirement already satisfied, skipping upgrade: sacremoses in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (0.0.46)\n",
      "Requirement already satisfied, skipping upgrade: filelock in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied, skipping upgrade: tokenizers<0.11,>=0.10.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (0.10.3)\n",
      "Requirement already satisfied, skipping upgrade: pyyaml>=5.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (5.3.1)\n",
      "Requirement already satisfied, skipping upgrade: dill in c:\\users\\hp\\anaconda3\\lib\\site-packages (from datasets) (0.3.4)\n",
      "Requirement already satisfied, skipping upgrade: aiohttp in c:\\users\\hp\\anaconda3\\lib\\site-packages (from datasets) (3.7.4)\n",
      "Requirement already satisfied, skipping upgrade: xxhash in c:\\users\\hp\\anaconda3\\lib\\site-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied, skipping upgrade: multiprocess in c:\\users\\hp\\anaconda3\\lib\\site-packages (from datasets) (0.70.12.2)\n",
      "Requirement already satisfied, skipping upgrade: pandas in c:\\users\\hp\\anaconda3\\lib\\site-packages (from datasets) (1.1.3)\n",
      "Requirement already satisfied, skipping upgrade: pyarrow!=4.0.0,>=1.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from datasets) (6.0.0)\n",
      "Requirement already satisfied, skipping upgrade: fsspec[http]>=2021.05.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from datasets) (2021.11.0)\n",
      "Requirement already satisfied, skipping upgrade: typing-extensions>=3.7.4.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.0.0)\n",
      "Requirement already satisfied, skipping upgrade: charset-normalizer~=2.0.0; python_version >= \"3\" in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied, skipping upgrade: idna<4,>=2.5; python_version >= \"3\" in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied, skipping upgrade: urllib3<1.27,>=1.21.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.7)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied, skipping upgrade: six in c:\\users\\hp\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (1.16.0)\n",
      "Requirement already satisfied, skipping upgrade: joblib in c:\\users\\hp\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (0.17.0)\n",
      "Requirement already satisfied, skipping upgrade: click in c:\\users\\hp\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied, skipping upgrade: chardet<5.0,>=2.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: multidict<7.0,>=4.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (5.1.0)\n",
      "Requirement already satisfied, skipping upgrade: yarl<2.0,>=1.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.6.3)\n",
      "Requirement already satisfied, skipping upgrade: attrs>=17.3.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (20.3.0)\n",
      "Requirement already satisfied, skipping upgrade: async-timeout<4.0,>=3.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (3.0.1)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.7.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.8.1)\n",
      "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pandas->datasets) (2020.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "#MODEL = \"cardiffnlp/twitter-roberta-base-sentiment\" \n",
    "MODEL = \"roberta-base\" # Will likely change\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "# Note: How we preprocess may depend on model we use to transfer. \n",
    "# This comes from https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment\n",
    "def preprocess(text):\n",
    "    new_text = []\n",
    "    for t in text.split(\" \"):\n",
    "        t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
    "        t = 'http' if t.startswith('http') else t\n",
    "        new_text.append(t)\n",
    "    return \" \".join(new_text)\n",
    "\n",
    "# Load data into numpy arrays\n",
    "X = np.array(emotion_df['tweet'])\n",
    "Y = np.array(emotion_df['category'])\n",
    "\n",
    "# Preprocess text\n",
    "for i in range(len(X)):\n",
    "    X[i] = preprocess(X[i])\n",
    "\n",
    "# Split into train/val/test sets\n",
    "TRAIN_PCT, VAL_PCT, TEST_PCT  = 0.6, 0.2, 0.2\n",
    "train_idx = int(TRAIN_PCT * len(X))\n",
    "val_idx = train_idx + int(VAL_PCT * len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0,  1039, 12105,  ...,     1,     1,     1],\n",
       "        [    0, 33543,    42,  ...,     1,     1,     1],\n",
       "        [    0, 10431,  7565,  ...,     1,     1,     1],\n",
       "        ...,\n",
       "        [    0,  8346,   127,  ...,     1,     1,     1],\n",
       "        [    0, 10836,    52,  ...,     1,     1,     1],\n",
       "        [    0,  1039, 12105,  ...,     1,     1,     1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, Y_train = X[:train_idx], Y[:train_idx]\n",
    "X_val, Y_val = X[train_idx:val_idx], Y[train_idx:val_idx]\n",
    "X_test, Y_test = X[val_idx:], Y[val_idx:]\n",
    "\n",
    "# Tokenize the data\n",
    "X_train_enc = tokenizer(list(X_train), return_tensors='pt', padding=True, truncation=True)\n",
    "X_val_enc = tokenizer(list(X_val), return_tensors='pt', padding=True, truncation=True)\n",
    "X_test_enc = tokenizer(list(X_test), return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "X_train_enc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: define our machine learning model, from our discussion it we can try deep learning models\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import transformers\n",
    "import torch\n",
    "from torch.utils.data import (\n",
    "    Dataset, \n",
    "    DataLoader, \n",
    "    RandomSampler, \n",
    "    SequentialSampler\n",
    ")\n",
    "\n",
    "import math \n",
    "from transformers import  (\n",
    "    BertPreTrainedModel, \n",
    "    RobertaConfig, \n",
    "    RobertaTokenizerFast\n",
    ")\n",
    "\n",
    "from transformers.optimization import (\n",
    "    AdamW, \n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "\n",
    "from scipy.special import softmax\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    matthews_corrcoef,\n",
    "    roc_curve,\n",
    "    auc,\n",
    "    average_precision_score,\n",
    ")\n",
    "\n",
    "from transformers.models.roberta.modeling_roberta import (\n",
    "    RobertaClassificationHead,\n",
    "    RobertaConfig,\n",
    "    RobertaModel,\n",
    ")\n",
    "model_name = 'roberta-base'\n",
    "num_labels = 4\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "tokenizer_name = model_name\n",
    "\n",
    "max_seq_length = 128 \n",
    "train_batch_size = 8\n",
    "test_batch_size = 8\n",
    "warmup_ratio = 0.06\n",
    "weight_decay=0.0\n",
    "gradient_accumulation_steps = 1\n",
    "num_train_epochs = 10\n",
    "learning_rate = 1e-05\n",
    "adam_epsilon = 1e-08\n",
    "\n",
    "class RobertaClassification(BertPreTrainedModel):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super(RobertaClassification, self).__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.roberta = RobertaModel(config)\n",
    "        self.classifier = RobertaClassificationHead(config)\n",
    "        \n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, labels):\n",
    "        outputs = self.roberta(input_ids,attention_mask=attention_mask)\n",
    "        sequence_output = outputs[0]\n",
    "        logits = self.classifier(sequence_output)\n",
    "\n",
    "        outputs = (logits,) + outputs[2:]\n",
    "        \n",
    "        loss_fct = CrossEntropyLoss()\n",
    "        loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "        outputs = (loss,) + outputs\n",
    "\n",
    "        return outputs  # (loss), logits, (hidden_states), (attentions)\n",
    "config_class = RobertaConfig\n",
    "model_class = RobertaClassification\n",
    "tokenizer_class = RobertaTokenizerFast\n",
    "config = config_class.from_pretrained(model_name, num_labels=num_labels)\n",
    "\n",
    "model = model_class.from_pretrained(model_name, config=config)\n",
    "print('Model=\\n',model,'\\n')\n",
    "\n",
    "tokenizer = tokenizer_class.from_pretrained(tokenizer_name, do_lower_case=False)\n",
    "print('Tokenizer=',tokenizer,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyClassificationDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data,y):\n",
    "        text = data\n",
    "        labels=y\n",
    "        self.examples = text\n",
    "        targets = tr.transform(labels)\n",
    "        self.labels = torch.as_tensor(targets,dtype=torch.long)\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples[\"input_ids\"])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return {key: self.examples[key][index] for key in self.examples}, self.labels[index]\n",
    "\n",
    "#X_train, Y_train = X[:train_idx], Y[:train_idx]\n",
    "#X_val, Y_val = X[train_idx:val_idx], Y[train_idx:val_idx]\n",
    "#X_test, Y_test = X[val_idx:], Y[val_idx:]\n",
    "\n",
    "train_dataset = MyClassificationDataset(X_train_enc,Y_train)\n",
    "val_dataset = MyClassificationDataset(X_val_enc, Y_val)\n",
    "test_dataset = MyClassificationDataset(X_test_enc, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    0, 14287,    42,  2770,   697,     4,   352,  2308,    30,   787,\n",
      "         12105,   849,   462,  6608,   849, 13792,  3435,     2,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1],\n",
      "        [    0, 19993,  2235,    35,   128,  8275,    45,  1733, 31554,     9,\n",
      "           608,   205,   955,   787, 12105,   849,   347, 21525,  9029,     2,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1],\n",
      "        [    0,   170,   214,   182,  3610,   849,   438, 19519,    10,  1086,\n",
      "          1546,  1044,    13,   849, 36564,   246,   417,   716,    15,   849,\n",
      "         46614, 11655, 11745,     4,   849,   571,  7486,  3623,   849,  2028,\n",
      "          2550,  3623,   849,   246,   424, 33838, 14699,  1437,   849,  2670,\n",
      "             2,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1],\n",
      "        [    0,  1039, 12105, 29784,     4,  3437,    24,    77,    47, 29223,\n",
      "           877,  7056,     4,  7238,    62,     5,   205,   173,     4, 36293,\n",
      "             2,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1],\n",
      "        [    0,  1039, 12105,  7995,   546,  4520,     4,  3830,  3548,   546,\n",
      "           101,  3830,  3548,     4,     2,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1],\n",
      "        [    0,  1039, 12105, 11380,    10,  5231,  6515,  5231,  2800,  1901,\n",
      "          7586,     2,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1],\n",
      "        [    0, 14287,    42,  2770,   697,     4,   352,  2308,    30,   787,\n",
      "         12105,  1437,   849, 13792,  3435,     2,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1],\n",
      "        [    0, 14699,     7,   213,   478,    62,     5,  5560,   111,    38,\n",
      "            33,    10,  9869, 32546,  3850,     9,  1040, 13747,     7,  5555,\n",
      "            42,   662,   734,  1437,   849, 17788,   849, 30164,     2,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "       device='cuda:0'), 'labels': tensor([2, 3, 1, 0, 2, 1, 2, 2], device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "train_batch_size = 8\n",
    "val_batch_size = 8\n",
    "test_batch_size = 8\n",
    "\n",
    "def get_inputs_dict(batch):\n",
    "    inputs = {key: value.squeeze(1).to(device) for key, value in batch[0].items()}\n",
    "    inputs[\"labels\"] = batch[1].to(device)\n",
    "    return inputs\n",
    "\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "train_dataloader = DataLoader(train_dataset,sampler=train_sampler,batch_size=train_batch_size)\n",
    "\n",
    "val_sampler = SequentialSampler(val_dataset)\n",
    "val_dataloader = DataLoader(val_dataset, sampler=val_sampler, batch_size=val_batch_size)\n",
    "\n",
    "#Extract a batch as sanity-check\n",
    "batch = get_inputs_dict(next(iter(train_dataloader)))\n",
    "input_ids = batch['input_ids'].to(device)\n",
    "attention_mask = batch['attention_mask'].to(device)\n",
    "labels = batch['labels'].to(device)\n",
    "\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_total = len(train_dataloader) // gradient_accumulation_steps * num_train_epochs\n",
    "optimizer_grouped_parameters = []\n",
    "custom_parameter_names = set()\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "optimizer_grouped_parameters.extend(\n",
    "    [\n",
    "        {\n",
    "            \"params\": [\n",
    "                p\n",
    "                for n, p in model.named_parameters()\n",
    "                if n not in custom_parameter_names and not any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            \"weight_decay\": weight_decay,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [\n",
    "                p\n",
    "                for n, p in model.named_parameters()\n",
    "                if n not in custom_parameter_names and any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]\n",
    ")\n",
    "\n",
    "warmup_steps = math.ceil(t_total * warmup_ratio)\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate, eps=adam_epsilon)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=t_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: train our model using the loaded data\n",
    "model.to(device)\n",
    "\n",
    "model.zero_grad()\n",
    "\n",
    "for epoch in range(num_train_epochs):\n",
    "\n",
    "    model.train()\n",
    "    epoch_loss = []\n",
    "    \n",
    "    for batch in train_dataloader:\n",
    "        batch = get_inputs_dict(batch)\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs[0]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        model.zero_grad()\n",
    "        epoch_loss.append(loss.item())\n",
    "        \n",
    "    #evaluate model with test_df at the end of the epoch.\n",
    "    eval_loss = 0.0\n",
    "    nb_eval_steps = 0\n",
    "    n_batches = len(val_dataloader)\n",
    "    preds = np.empty((len(val_dataset), num_labels))\n",
    "    out_label_ids = np.empty((len(val_dataset)))\n",
    "    model.eval()\n",
    "    \n",
    "    for i,test_batch in enumerate(val_dataloader):\n",
    "        with torch.no_grad():\n",
    "            test_batch = get_inputs_dict(test_batch)\n",
    "            input_ids = test_batch['input_ids'].to(device)\n",
    "            attention_mask = test_batch['attention_mask'].to(device)\n",
    "            labels = test_batch['labels'].to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            tmp_eval_loss, logits = outputs[:2]\n",
    "            eval_loss += tmp_eval_loss.item()\n",
    "            \n",
    "        nb_eval_steps += 1\n",
    "        start_index = test_batch_size * i\n",
    "        end_index = start_index + test_batch_size if i != (n_batches - 1) else len(test_dataset)\n",
    "        preds[start_index:end_index] = logits.detach().cpu().numpy()\n",
    "        out_label_ids[start_index:end_index] = test_batch[\"labels\"].detach().cpu().numpy()\n",
    "        \n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    model_outputs = preds\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "    #result, wrong = compute_metrics(preds, model_outputs, out_label_ids)\n",
    "    \n",
    "    print('epoch',epoch,'Training avg loss',np.mean(epoch_loss))\n",
    "    print('epoch',epoch,'Testing  avg loss',eval_loss)\n",
    "    #print(result)\n",
    "    #print_confusion_matrix(result)\n",
    "    print('---------------------------------------------------\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: evaluate using accuracy to see how well our model is performing on the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: predict the submission data "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
