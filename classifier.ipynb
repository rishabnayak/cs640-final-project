{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "os.environ['OMP_NUM_THREADS'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imagename</th>\n",
       "      <th>postid</th>\n",
       "      <th>dataset</th>\n",
       "      <th>dataset.1</th>\n",
       "      <th>postdate</th>\n",
       "      <th>Contents</th>\n",
       "      <th>url</th>\n",
       "      <th>Q1_pertain_to_covid</th>\n",
       "      <th>Q2_cetegory</th>\n",
       "      <th>Q2A_Type of Human</th>\n",
       "      <th>...</th>\n",
       "      <th>Q8_threat_covid</th>\n",
       "      <th>Q9_susceptibility_covid</th>\n",
       "      <th>Q9A_Asian responsible for the covid</th>\n",
       "      <th>Q10_solution_present</th>\n",
       "      <th>Q11_recommended_solution</th>\n",
       "      <th>misinformation</th>\n",
       "      <th>Q12 Presence of conspiracy theory</th>\n",
       "      <th>Q12-Others</th>\n",
       "      <th>Q13.  Image of plague doctor costume</th>\n",
       "      <th>Note</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B-A5FIIIEKJ</td>\n",
       "      <td>5592</td>\n",
       "      <td>middle</td>\n",
       "      <td>2</td>\n",
       "      <td>3/21/20</td>\n",
       "      <td>#covid #covid2020 #covidvirus #virus #coronava...</td>\n",
       "      <td>https://www.instagram.com/p/B-A5FIIIEKJ/</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>99</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Stay home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B-A74YQn_4a</td>\n",
       "      <td>5593</td>\n",
       "      <td>middle</td>\n",
       "      <td>2</td>\n",
       "      <td>3/21/20</td>\n",
       "      <td>Well this is the final mural of my trip in Aus...</td>\n",
       "      <td>https://www.instagram.com/p/B-A74YQn_4a/</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>99</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>99</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B-AajnDp6GQ</td>\n",
       "      <td>5575</td>\n",
       "      <td>middle</td>\n",
       "      <td>2</td>\n",
       "      <td>3/21/20</td>\n",
       "      <td>Chegamos !!! V√£o seguindo o movimento... Tem m...</td>\n",
       "      <td>https://www.instagram.com/p/B-AajnDp6GQ/</td>\n",
       "      <td>2</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>...</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>0</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B-AarR4pUQA</td>\n",
       "      <td>5576</td>\n",
       "      <td>middle</td>\n",
       "      <td>2</td>\n",
       "      <td>3/21/20</td>\n",
       "      <td>üòªüòªüòªüòªüòª</td>\n",
       "      <td>https://www.instagram.com/p/B-AarR4pUQA/</td>\n",
       "      <td>2</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>...</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>0</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B-AE1y3HD-Z</td>\n",
       "      <td>5550</td>\n",
       "      <td>middle</td>\n",
       "      <td>2</td>\n",
       "      <td>3/21/20</td>\n",
       "      <td>EN MI DOMICILIO üè°\\r\\r\\r\\r\\n#quedateencasa\\r\\r\\...</td>\n",
       "      <td>https://www.instagram.com/p/B-AE1y3HD-Z/</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>99</td>\n",
       "      <td>2.0</td>\n",
       "      <td>mask; Stay home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9643</th>\n",
       "      <td>CEZyheBgtMd</td>\n",
       "      <td>8402</td>\n",
       "      <td>second</td>\n",
       "      <td>3</td>\n",
       "      <td>8/27/20</td>\n",
       "      <td>#water #foryou #followforfollowback #photograp...</td>\n",
       "      <td>https://www.instagram.com/p/CEZyheBgtMd/</td>\n",
       "      <td>2</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>...</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>0</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9644</th>\n",
       "      <td>CEZYoyRluKG</td>\n",
       "      <td>8352</td>\n",
       "      <td>second</td>\n",
       "      <td>3</td>\n",
       "      <td>8/27/20</td>\n",
       "      <td>#like4likes #20likes #tagforlikes #instalikes ...</td>\n",
       "      <td>https://www.instagram.com/p/CEZYoyRluKG/</td>\n",
       "      <td>2</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>...</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>0</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9645</th>\n",
       "      <td>CEZz-solaUF</td>\n",
       "      <td>8403</td>\n",
       "      <td>second</td>\n",
       "      <td>3</td>\n",
       "      <td>8/27/20</td>\n",
       "      <td>üôàü•∞üòçüëâüèΩ @love_serie_karma #daancorona #daancoron...</td>\n",
       "      <td>https://www.instagram.com/p/CEZz-solaUF/</td>\n",
       "      <td>2</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>...</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>0</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9646</th>\n",
       "      <td>CEZZ4P4j3rh</td>\n",
       "      <td>8354</td>\n",
       "      <td>second</td>\n",
       "      <td>3</td>\n",
       "      <td>8/27/20</td>\n",
       "      <td>üí•üö® ùóôùóîùóüùóü ùóúùó¶ ùóñùó¢ùó†ùóúùó°ùóö üö®üí•‚Å£\\r\\r\\r\\r\\r\\n‚Å£\\r\\r\\r\\r\\r\\n...</td>\n",
       "      <td>https://www.instagram.com/p/CEZZ4P4j3rh/</td>\n",
       "      <td>2</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>...</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>0</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9647</th>\n",
       "      <td>CEZZyqshFLq</td>\n",
       "      <td>8353</td>\n",
       "      <td>second</td>\n",
       "      <td>3</td>\n",
       "      <td>8/27/20</td>\n",
       "      <td>Follow me @kestine_kylie \\r\\r\\r\\r\\n.\\r\\r\\r\\r\\n...</td>\n",
       "      <td>https://www.instagram.com/p/CEZZyqshFLq/</td>\n",
       "      <td>2</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>...</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>0</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9648 rows √ó 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        imagename  postid dataset  dataset.1 postdate  \\\n",
       "0     B-A5FIIIEKJ    5592  middle          2  3/21/20   \n",
       "1     B-A74YQn_4a    5593  middle          2  3/21/20   \n",
       "2     B-AajnDp6GQ    5575  middle          2  3/21/20   \n",
       "3     B-AarR4pUQA    5576  middle          2  3/21/20   \n",
       "4     B-AE1y3HD-Z    5550  middle          2  3/21/20   \n",
       "...           ...     ...     ...        ...      ...   \n",
       "9643  CEZyheBgtMd    8402  second          3  8/27/20   \n",
       "9644  CEZYoyRluKG    8352  second          3  8/27/20   \n",
       "9645  CEZz-solaUF    8403  second          3  8/27/20   \n",
       "9646  CEZZ4P4j3rh    8354  second          3  8/27/20   \n",
       "9647  CEZZyqshFLq    8353  second          3  8/27/20   \n",
       "\n",
       "                                               Contents  \\\n",
       "0     #covid #covid2020 #covidvirus #virus #coronava...   \n",
       "1     Well this is the final mural of my trip in Aus...   \n",
       "2     Chegamos !!! V√£o seguindo o movimento... Tem m...   \n",
       "3                                                 üòªüòªüòªüòªüòª   \n",
       "4     EN MI DOMICILIO üè°\\r\\r\\r\\r\\n#quedateencasa\\r\\r\\...   \n",
       "...                                                 ...   \n",
       "9643  #water #foryou #followforfollowback #photograp...   \n",
       "9644  #like4likes #20likes #tagforlikes #instalikes ...   \n",
       "9645  üôàü•∞üòçüëâüèΩ @love_serie_karma #daancorona #daancoron...   \n",
       "9646  üí•üö® ùóôùóîùóüùóü ùóúùó¶ ùóñùó¢ùó†ùóúùó°ùóö üö®üí•‚Å£\\r\\r\\r\\r\\r\\n‚Å£\\r\\r\\r\\r\\r\\n...   \n",
       "9647  Follow me @kestine_kylie \\r\\r\\r\\r\\n.\\r\\r\\r\\r\\n...   \n",
       "\n",
       "                                           url  Q1_pertain_to_covid  \\\n",
       "0     https://www.instagram.com/p/B-A5FIIIEKJ/                    1   \n",
       "1     https://www.instagram.com/p/B-A74YQn_4a/                    1   \n",
       "2     https://www.instagram.com/p/B-AajnDp6GQ/                    2   \n",
       "3     https://www.instagram.com/p/B-AarR4pUQA/                    2   \n",
       "4     https://www.instagram.com/p/B-AE1y3HD-Z/                    1   \n",
       "...                                        ...                  ...   \n",
       "9643  https://www.instagram.com/p/CEZyheBgtMd/                    2   \n",
       "9644  https://www.instagram.com/p/CEZYoyRluKG/                    2   \n",
       "9645  https://www.instagram.com/p/CEZz-solaUF/                    2   \n",
       "9646  https://www.instagram.com/p/CEZZ4P4j3rh/                    2   \n",
       "9647  https://www.instagram.com/p/CEZZyqshFLq/                    2   \n",
       "\n",
       "      Q2_cetegory  Q2A_Type of Human  ...  Q8_threat_covid  \\\n",
       "0               1                  1  ...                1   \n",
       "1               1                  1  ...                1   \n",
       "2              99                 99  ...               99   \n",
       "3              99                 99  ...               99   \n",
       "4               1                  1  ...                1   \n",
       "...           ...                ...  ...              ...   \n",
       "9643           99                 99  ...               99   \n",
       "9644           99                 99  ...               99   \n",
       "9645           99                 99  ...               99   \n",
       "9646           99                 99  ...               99   \n",
       "9647           99                 99  ...               99   \n",
       "\n",
       "      Q9_susceptibility_covid  Q9A_Asian responsible for the covid  \\\n",
       "0                           1                                    2   \n",
       "1                           1                                    2   \n",
       "2                          99                                   99   \n",
       "3                          99                                   99   \n",
       "4                           1                                    2   \n",
       "...                       ...                                  ...   \n",
       "9643                       99                                   99   \n",
       "9644                       99                                   99   \n",
       "9645                       99                                   99   \n",
       "9646                       99                                   99   \n",
       "9647                       99                                   99   \n",
       "\n",
       "      Q10_solution_present  Q11_recommended_solution  misinformation  \\\n",
       "0                        1                         1               0   \n",
       "1                        2                        99               0   \n",
       "2                       99                        99               0   \n",
       "3                       99                        99               0   \n",
       "4                        1                         1               0   \n",
       "...                    ...                       ...             ...   \n",
       "9643                    99                        99               0   \n",
       "9644                    99                        99               0   \n",
       "9645                    99                        99               0   \n",
       "9646                    99                        99               0   \n",
       "9647                    99                        99               0   \n",
       "\n",
       "      Q12 Presence of conspiracy theory  Q12-Others  \\\n",
       "0                                     2          99   \n",
       "1                                     2          99   \n",
       "2                                    99          99   \n",
       "3                                    99          99   \n",
       "4                                     2          99   \n",
       "...                                 ...         ...   \n",
       "9643                                 99          99   \n",
       "9644                                 99          99   \n",
       "9645                                 99          99   \n",
       "9646                                 99          99   \n",
       "9647                                 99          99   \n",
       "\n",
       "      Q13.  Image of plague doctor costume             Note  \n",
       "0                                      2.0        Stay home  \n",
       "1                                      2.0              NaN  \n",
       "2                                     99.0              NaN  \n",
       "3                                     99.0              NaN  \n",
       "4                                      2.0  mask; Stay home  \n",
       "...                                    ...              ...  \n",
       "9643                                  99.0              NaN  \n",
       "9644                                  99.0              NaN  \n",
       "9645                                  99.0              NaN  \n",
       "9646                                  99.0              NaN  \n",
       "9647                                  99.0              NaN  \n",
       "\n",
       "[9648 rows x 26 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "ins_df = pd.read_csv('data/instagram_data.csv')\n",
    "ins_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>@WatcherDev @WarcraftDevs put so much artefact...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>@BonesARP 'That is a disappointment.'\\n\\nHe fa...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>@VodafoneUKhelp @VodafoneUK wow!! My bill is √Ç...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>575</th>\n",
       "      <td>Nahhhhhh @konanplaydirty snap story has got me...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>@morgannbroom yes √¢¬ù¬§√Ø¬∏¬è√¢¬ù¬§√Ø¬∏¬è &amp;amp; cheering ...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444</th>\n",
       "      <td>@CBSBigBrother never bring back Meech and Brid...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>761</th>\n",
       "      <td>blues bar</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>@AaronGoodwin seriously dude buy some bubble t...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>@DouthatNYT @jbouie @DamonLinker Yes, it is ba...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>707</th>\n",
       "      <td>No one wants to win the wild card because you ...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3613 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 tweet category\n",
       "144  @WatcherDev @WarcraftDevs put so much artefact...    anger\n",
       "469  @BonesARP 'That is a disappointment.'\\n\\nHe fa...  sadness\n",
       "179  @VodafoneUKhelp @VodafoneUK wow!! My bill is √Ç...    anger\n",
       "575  Nahhhhhh @konanplaydirty snap story has got me...    anger\n",
       "81   @morgannbroom yes √¢¬ù¬§√Ø¬∏¬è√¢¬ù¬§√Ø¬∏¬è &amp; cheering ...      joy\n",
       "..                                                 ...      ...\n",
       "444  @CBSBigBrother never bring back Meech and Brid...    anger\n",
       "761                                          blues bar  sadness\n",
       "476  @AaronGoodwin seriously dude buy some bubble t...    anger\n",
       "310  @DouthatNYT @jbouie @DamonLinker Yes, it is ba...    anger\n",
       "707  No one wants to win the wild card because you ...  sadness\n",
       "\n",
       "[3613 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "anger_df = pd.read_csv('data/twitter/anger.tsv', sep='\\t').drop(columns=['index', 'intensity'])\n",
    "fear_df = pd.read_csv('data/twitter/fear.tsv', sep='\\t').drop(columns=['index', 'intensity'])\n",
    "joy_df = pd.read_csv('data/twitter/joy.tsv', sep='\\t').drop(columns=['index', 'intensity'])\n",
    "sadness_df = pd.read_csv('data/twitter/sadness.tsv', sep='\\t').drop(columns=['index', 'intensity'])\n",
    "\n",
    "emotion_df = pd.concat([anger_df, fear_df, joy_df, sadness_df])\n",
    "emotion_df = shuffle(emotion_df)\n",
    "\n",
    "emotion_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: transformers in c:\\users\\hp\\anaconda3\\lib\\site-packages (4.12.5)\n",
      "Requirement already up-to-date: datasets in c:\\users\\hp\\anaconda3\\lib\\site-packages (1.15.1)\n",
      "Requirement already satisfied, skipping upgrade: huggingface-hub<1.0,>=0.1.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (0.1.2)\n",
      "Requirement already satisfied, skipping upgrade: requests in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (2.26.0)\n",
      "Requirement already satisfied, skipping upgrade: regex!=2019.12.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (2020.10.15)\n",
      "Requirement already satisfied, skipping upgrade: packaging>=20.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (20.4)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (1.21.4)\n",
      "Requirement already satisfied, skipping upgrade: tqdm>=4.27 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (4.50.2)\n",
      "Requirement already satisfied, skipping upgrade: sacremoses in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (0.0.46)\n",
      "Requirement already satisfied, skipping upgrade: filelock in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied, skipping upgrade: tokenizers<0.11,>=0.10.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (0.10.3)\n",
      "Requirement already satisfied, skipping upgrade: pyyaml>=5.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (5.3.1)\n",
      "Requirement already satisfied, skipping upgrade: dill in c:\\users\\hp\\anaconda3\\lib\\site-packages (from datasets) (0.3.4)\n",
      "Requirement already satisfied, skipping upgrade: aiohttp in c:\\users\\hp\\anaconda3\\lib\\site-packages (from datasets) (3.7.4)\n",
      "Requirement already satisfied, skipping upgrade: xxhash in c:\\users\\hp\\anaconda3\\lib\\site-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied, skipping upgrade: multiprocess in c:\\users\\hp\\anaconda3\\lib\\site-packages (from datasets) (0.70.12.2)\n",
      "Requirement already satisfied, skipping upgrade: pandas in c:\\users\\hp\\anaconda3\\lib\\site-packages (from datasets) (1.1.3)\n",
      "Requirement already satisfied, skipping upgrade: pyarrow!=4.0.0,>=1.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from datasets) (6.0.0)\n",
      "Requirement already satisfied, skipping upgrade: fsspec[http]>=2021.05.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from datasets) (2021.11.0)\n",
      "Requirement already satisfied, skipping upgrade: typing-extensions>=3.7.4.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.0.0)\n",
      "Requirement already satisfied, skipping upgrade: charset-normalizer~=2.0.0; python_version >= \"3\" in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied, skipping upgrade: idna<4,>=2.5; python_version >= \"3\" in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied, skipping upgrade: urllib3<1.27,>=1.21.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.7)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied, skipping upgrade: six in c:\\users\\hp\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (1.16.0)\n",
      "Requirement already satisfied, skipping upgrade: joblib in c:\\users\\hp\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (0.17.0)\n",
      "Requirement already satisfied, skipping upgrade: click in c:\\users\\hp\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied, skipping upgrade: chardet<5.0,>=2.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: multidict<7.0,>=4.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (5.1.0)\n",
      "Requirement already satisfied, skipping upgrade: yarl<2.0,>=1.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.6.3)\n",
      "Requirement already satisfied, skipping upgrade: attrs>=17.3.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (20.3.0)\n",
      "Requirement already satisfied, skipping upgrade: async-timeout<4.0,>=3.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (3.0.1)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.7.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.8.1)\n",
      "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pandas->datasets) (2020.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "MODEL = \"cardiffnlp/twitter-roberta-base-sentiment\" \n",
    "#MODEL = \"roberta-base\" # Will likely change\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "# Note: How we preprocess may depend on model we use to transfer. \n",
    "# This comes from https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment\n",
    "def preprocess(text):\n",
    "    new_text = []\n",
    "    for t in text.split(\" \"):\n",
    "        t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
    "        t = 'http' if t.startswith('http') else t\n",
    "        new_text.append(t)\n",
    "    return \" \".join(new_text)\n",
    "\n",
    "# Load data into numpy arrays\n",
    "X = np.array(emotion_df['tweet'])\n",
    "Y = np.array(emotion_df['category'])\n",
    "\n",
    "# Preprocess text\n",
    "for i in range(len(X)):\n",
    "    X[i] = preprocess(X[i])\n",
    "\n",
    "# Split into train/val/test sets\n",
    "TRAIN_PCT, VAL_PCT, TEST_PCT  = 0.6, 0.2, 0.2\n",
    "train_idx = int(TRAIN_PCT * len(X))\n",
    "val_idx = train_idx + int(VAL_PCT * len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0,  1039, 12105,  ...,     1,     1,     1],\n",
       "        [    0,  1039, 12105,  ...,     1,     1,     1],\n",
       "        [    0,  1039, 12105,  ...,     1,     1,     1],\n",
       "        ...,\n",
       "        [    0,   387,  1322,  ...,     1,     1,     1],\n",
       "        [    0,   510, 11842,  ...,     1,     1,     1],\n",
       "        [    0,  1039, 12105,  ...,     1,     1,     1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, Y_train = X[:train_idx], Y[:train_idx]\n",
    "X_val, Y_val = X[train_idx:val_idx], Y[train_idx:val_idx]\n",
    "X_test, Y_test = X[val_idx:], Y[val_idx:]\n",
    "\n",
    "# Tokenize the data\n",
    "X_train_enc = tokenizer(list(X_train), return_tensors='pt', padding=True, truncation=True)\n",
    "X_val_enc = tokenizer(list(X_val), return_tensors='pt', padding=True, truncation=True)\n",
    "X_test_enc = tokenizer(list(X_test), return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "X_train_enc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaClassification: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model=\n",
      " RobertaClassification(\n",
      "  (roberta): RobertaModel(\n",
      "    (embeddings): RobertaEmbeddings(\n",
      "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
      "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "      (token_type_embeddings): Embedding(1, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): RobertaEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): RobertaPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (classifier): RobertaClassificationHead(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (out_proj): Linear(in_features=768, out_features=4, bias=True)\n",
      "  )\n",
      ") \n",
      "\n",
      "Tokenizer= PreTrainedTokenizerFast(name_or_path='roberta-base', vocab_size=50265, model_max_len=512, is_fast=True, padding_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False)}) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: define our machine learning model, from our discussion it we can try deep learning models\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import transformers\n",
    "import torch\n",
    "from torch.utils.data import (\n",
    "    Dataset, \n",
    "    DataLoader, \n",
    "    RandomSampler, \n",
    "    SequentialSampler\n",
    ")\n",
    "\n",
    "import math \n",
    "from transformers import  (\n",
    "    BertPreTrainedModel, \n",
    "    RobertaConfig, \n",
    "    RobertaTokenizerFast\n",
    ")\n",
    "\n",
    "from transformers.optimization import (\n",
    "    AdamW, \n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "\n",
    "from scipy.special import softmax\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    matthews_corrcoef,\n",
    "    roc_curve,\n",
    "    auc,\n",
    "    average_precision_score,\n",
    ")\n",
    "\n",
    "from transformers.models.roberta.modeling_roberta import (\n",
    "    RobertaClassificationHead,\n",
    "    RobertaConfig,\n",
    "    RobertaModel,\n",
    ")\n",
    "model_name = 'roberta-base'\n",
    "num_labels = 4\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "tokenizer_name = model_name\n",
    "\n",
    "max_seq_length = 128 \n",
    "train_batch_size = 8\n",
    "test_batch_size = 8\n",
    "warmup_ratio = 0.06\n",
    "weight_decay=0.0\n",
    "gradient_accumulation_steps = 1\n",
    "num_train_epochs = 10\n",
    "learning_rate = 1e-05\n",
    "adam_epsilon = 1e-08\n",
    "\n",
    "class RobertaClassification(BertPreTrainedModel):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super(RobertaClassification, self).__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.roberta = RobertaModel(config)\n",
    "        self.classifier = RobertaClassificationHead(config)\n",
    "        \n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, labels):\n",
    "        outputs = self.roberta(input_ids,attention_mask=attention_mask)\n",
    "        sequence_output = outputs[0]\n",
    "        logits = self.classifier(sequence_output)\n",
    "\n",
    "        outputs = (logits,) + outputs[2:]\n",
    "        \n",
    "        loss_fct = CrossEntropyLoss()\n",
    "        loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "        outputs = (loss,) + outputs\n",
    "\n",
    "        return outputs  # (loss), logits, (hidden_states), (attentions)\n",
    "config_class = RobertaConfig\n",
    "model_class = RobertaClassification\n",
    "tokenizer_class = RobertaTokenizerFast\n",
    "config = config_class.from_pretrained(model_name, num_labels=num_labels)\n",
    "\n",
    "model = model_class.from_pretrained(model_name, config=config)\n",
    "print('Model=\\n',model,'\\n')\n",
    "\n",
    "tokenizer = tokenizer_class.from_pretrained(tokenizer_name, do_lower_case=False)\n",
    "print('Tokenizer=',tokenizer,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-72c6c0d715d8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;31m#X_test, Y_test = X[val_idx:], Y[val_idx:]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[0mtrain_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMyClassificationDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_enc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[0mval_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMyClassificationDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val_enc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[0mtest_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMyClassificationDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_enc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-72c6c0d715d8>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, y)\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexamples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mtargets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tr' is not defined"
     ]
    }
   ],
   "source": [
    "class MyClassificationDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data,y):\n",
    "        text = data\n",
    "        labels=y\n",
    "        self.examples = text\n",
    "        targets = tr.transform(labels)\n",
    "        self.labels = torch.as_tensor(targets,dtype=torch.long)\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples[\"input_ids\"])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return {key: self.examples[key][index] for key in self.examples}, self.labels[index]\n",
    "\n",
    "#X_train, Y_train = X[:train_idx], Y[:train_idx]\n",
    "#X_val, Y_val = X[train_idx:val_idx], Y[train_idx:val_idx]\n",
    "#X_test, Y_test = X[val_idx:], Y[val_idx:]\n",
    "\n",
    "train_dataset = MyClassificationDataset(X_train_enc,Y_train)\n",
    "val_dataset = MyClassificationDataset(X_val_enc, Y_val)\n",
    "test_dataset = MyClassificationDataset(X_test_enc, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 8\n",
    "val_batch_size = 8\n",
    "test_batch_size = 8\n",
    "\n",
    "def get_inputs_dict(batch):\n",
    "    inputs = {key: value.squeeze(1).to(device) for key, value in batch[0].items()}\n",
    "    inputs[\"labels\"] = batch[1].to(device)\n",
    "    return inputs\n",
    "\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "train_dataloader = DataLoader(train_dataset,sampler=train_sampler,batch_size=train_batch_size)\n",
    "\n",
    "val_sampler = SequentialSampler(val_dataset)\n",
    "val_dataloader = DataLoader(val_dataset, sampler=val_sampler, batch_size=val_batch_size)\n",
    "\n",
    "#Extract a batch as sanity-check\n",
    "batch = get_inputs_dict(next(iter(train_dataloader)))\n",
    "input_ids = batch['input_ids'].to(device)\n",
    "attention_mask = batch['attention_mask'].to(device)\n",
    "labels = batch['labels'].to(device)\n",
    "\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_total = len(train_dataloader) // gradient_accumulation_steps * num_train_epochs\n",
    "optimizer_grouped_parameters = []\n",
    "custom_parameter_names = set()\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "optimizer_grouped_parameters.extend(\n",
    "    [\n",
    "        {\n",
    "            \"params\": [\n",
    "                p\n",
    "                for n, p in model.named_parameters()\n",
    "                if n not in custom_parameter_names and not any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            \"weight_decay\": weight_decay,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [\n",
    "                p\n",
    "                for n, p in model.named_parameters()\n",
    "                if n not in custom_parameter_names and any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]\n",
    ")\n",
    "\n",
    "warmup_steps = math.ceil(t_total * warmup_ratio)\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate, eps=adam_epsilon)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=t_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: train our model using the loaded data\n",
    "model.to(device)\n",
    "\n",
    "model.zero_grad()\n",
    "\n",
    "for epoch in range(num_train_epochs):\n",
    "\n",
    "    model.train()\n",
    "    epoch_loss = []\n",
    "    \n",
    "    for batch in train_dataloader:\n",
    "        batch = get_inputs_dict(batch)\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs[0]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        model.zero_grad()\n",
    "        epoch_loss.append(loss.item())\n",
    "        \n",
    "    #evaluate model with test_df at the end of the epoch.\n",
    "    eval_loss = 0.0\n",
    "    nb_eval_steps = 0\n",
    "    n_batches = len(val_dataloader)\n",
    "    preds = np.empty((len(val_dataset), num_labels))\n",
    "    out_label_ids = np.empty((len(val_dataset)))\n",
    "    model.eval()\n",
    "    \n",
    "    for i,test_batch in enumerate(val_dataloader):\n",
    "        with torch.no_grad():\n",
    "            test_batch = get_inputs_dict(test_batch)\n",
    "            input_ids = test_batch['input_ids'].to(device)\n",
    "            attention_mask = test_batch['attention_mask'].to(device)\n",
    "            labels = test_batch['labels'].to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            tmp_eval_loss, logits = outputs[:2]\n",
    "            eval_loss += tmp_eval_loss.item()\n",
    "            \n",
    "        nb_eval_steps += 1\n",
    "        start_index = test_batch_size * i\n",
    "        end_index = start_index + test_batch_size if i != (n_batches - 1) else len(test_dataset)\n",
    "        preds[start_index:end_index] = logits.detach().cpu().numpy()\n",
    "        out_label_ids[start_index:end_index] = test_batch[\"labels\"].detach().cpu().numpy()\n",
    "        \n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    model_outputs = preds\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "    #result, wrong = compute_metrics(preds, model_outputs, out_label_ids)\n",
    "    \n",
    "    print('epoch',epoch,'Training avg loss',np.mean(epoch_loss))\n",
    "    print('epoch',epoch,'Testing  avg loss',eval_loss)\n",
    "    #print(result)\n",
    "    #print_confusion_matrix(result)\n",
    "    print('---------------------------------------------------\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: evaluate using accuracy to see how well our model is performing on the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: predict the submission data "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
