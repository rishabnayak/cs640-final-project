{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "os.environ['OMP_NUM_THREADS'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imagename</th>\n",
       "      <th>postid</th>\n",
       "      <th>dataset</th>\n",
       "      <th>dataset.1</th>\n",
       "      <th>postdate</th>\n",
       "      <th>Contents</th>\n",
       "      <th>url</th>\n",
       "      <th>Q1_pertain_to_covid</th>\n",
       "      <th>Q2_cetegory</th>\n",
       "      <th>Q2A_Type of Human</th>\n",
       "      <th>...</th>\n",
       "      <th>Q8_threat_covid</th>\n",
       "      <th>Q9_susceptibility_covid</th>\n",
       "      <th>Q9A_Asian responsible for the covid</th>\n",
       "      <th>Q10_solution_present</th>\n",
       "      <th>Q11_recommended_solution</th>\n",
       "      <th>misinformation</th>\n",
       "      <th>Q12 Presence of conspiracy theory</th>\n",
       "      <th>Q12-Others</th>\n",
       "      <th>Q13.  Image of plague doctor costume</th>\n",
       "      <th>Note</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B-A5FIIIEKJ</td>\n",
       "      <td>5592</td>\n",
       "      <td>middle</td>\n",
       "      <td>2</td>\n",
       "      <td>3/21/20</td>\n",
       "      <td>#covid #covid2020 #covidvirus #virus #coronava...</td>\n",
       "      <td>https://www.instagram.com/p/B-A5FIIIEKJ/</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>99</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Stay home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B-A74YQn_4a</td>\n",
       "      <td>5593</td>\n",
       "      <td>middle</td>\n",
       "      <td>2</td>\n",
       "      <td>3/21/20</td>\n",
       "      <td>Well this is the final mural of my trip in Aus...</td>\n",
       "      <td>https://www.instagram.com/p/B-A74YQn_4a/</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>99</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>99</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B-AajnDp6GQ</td>\n",
       "      <td>5575</td>\n",
       "      <td>middle</td>\n",
       "      <td>2</td>\n",
       "      <td>3/21/20</td>\n",
       "      <td>Chegamos !!! V√£o seguindo o movimento... Tem m...</td>\n",
       "      <td>https://www.instagram.com/p/B-AajnDp6GQ/</td>\n",
       "      <td>2</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>...</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>0</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B-AarR4pUQA</td>\n",
       "      <td>5576</td>\n",
       "      <td>middle</td>\n",
       "      <td>2</td>\n",
       "      <td>3/21/20</td>\n",
       "      <td>üòªüòªüòªüòªüòª</td>\n",
       "      <td>https://www.instagram.com/p/B-AarR4pUQA/</td>\n",
       "      <td>2</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>...</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>0</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B-AE1y3HD-Z</td>\n",
       "      <td>5550</td>\n",
       "      <td>middle</td>\n",
       "      <td>2</td>\n",
       "      <td>3/21/20</td>\n",
       "      <td>EN MI DOMICILIO üè°\\r\\r\\r\\r\\n#quedateencasa\\r\\r\\...</td>\n",
       "      <td>https://www.instagram.com/p/B-AE1y3HD-Z/</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>99</td>\n",
       "      <td>2.0</td>\n",
       "      <td>mask; Stay home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9643</th>\n",
       "      <td>CEZyheBgtMd</td>\n",
       "      <td>8402</td>\n",
       "      <td>second</td>\n",
       "      <td>3</td>\n",
       "      <td>8/27/20</td>\n",
       "      <td>#water #foryou #followforfollowback #photograp...</td>\n",
       "      <td>https://www.instagram.com/p/CEZyheBgtMd/</td>\n",
       "      <td>2</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>...</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>0</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9644</th>\n",
       "      <td>CEZYoyRluKG</td>\n",
       "      <td>8352</td>\n",
       "      <td>second</td>\n",
       "      <td>3</td>\n",
       "      <td>8/27/20</td>\n",
       "      <td>#like4likes #20likes #tagforlikes #instalikes ...</td>\n",
       "      <td>https://www.instagram.com/p/CEZYoyRluKG/</td>\n",
       "      <td>2</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>...</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>0</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9645</th>\n",
       "      <td>CEZz-solaUF</td>\n",
       "      <td>8403</td>\n",
       "      <td>second</td>\n",
       "      <td>3</td>\n",
       "      <td>8/27/20</td>\n",
       "      <td>üôàü•∞üòçüëâüèΩ @love_serie_karma #daancorona #daancoron...</td>\n",
       "      <td>https://www.instagram.com/p/CEZz-solaUF/</td>\n",
       "      <td>2</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>...</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>0</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9646</th>\n",
       "      <td>CEZZ4P4j3rh</td>\n",
       "      <td>8354</td>\n",
       "      <td>second</td>\n",
       "      <td>3</td>\n",
       "      <td>8/27/20</td>\n",
       "      <td>üí•üö® ùóôùóîùóüùóü ùóúùó¶ ùóñùó¢ùó†ùóúùó°ùóö üö®üí•‚Å£\\r\\r\\r\\r\\r\\n‚Å£\\r\\r\\r\\r\\r\\n...</td>\n",
       "      <td>https://www.instagram.com/p/CEZZ4P4j3rh/</td>\n",
       "      <td>2</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>...</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>0</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9647</th>\n",
       "      <td>CEZZyqshFLq</td>\n",
       "      <td>8353</td>\n",
       "      <td>second</td>\n",
       "      <td>3</td>\n",
       "      <td>8/27/20</td>\n",
       "      <td>Follow me @kestine_kylie \\r\\r\\r\\r\\n.\\r\\r\\r\\r\\n...</td>\n",
       "      <td>https://www.instagram.com/p/CEZZyqshFLq/</td>\n",
       "      <td>2</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>...</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>0</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9589 rows √ó 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        imagename  postid dataset  dataset.1 postdate  \\\n",
       "0     B-A5FIIIEKJ    5592  middle          2  3/21/20   \n",
       "1     B-A74YQn_4a    5593  middle          2  3/21/20   \n",
       "2     B-AajnDp6GQ    5575  middle          2  3/21/20   \n",
       "3     B-AarR4pUQA    5576  middle          2  3/21/20   \n",
       "4     B-AE1y3HD-Z    5550  middle          2  3/21/20   \n",
       "...           ...     ...     ...        ...      ...   \n",
       "9643  CEZyheBgtMd    8402  second          3  8/27/20   \n",
       "9644  CEZYoyRluKG    8352  second          3  8/27/20   \n",
       "9645  CEZz-solaUF    8403  second          3  8/27/20   \n",
       "9646  CEZZ4P4j3rh    8354  second          3  8/27/20   \n",
       "9647  CEZZyqshFLq    8353  second          3  8/27/20   \n",
       "\n",
       "                                               Contents  \\\n",
       "0     #covid #covid2020 #covidvirus #virus #coronava...   \n",
       "1     Well this is the final mural of my trip in Aus...   \n",
       "2     Chegamos !!! V√£o seguindo o movimento... Tem m...   \n",
       "3                                                 üòªüòªüòªüòªüòª   \n",
       "4     EN MI DOMICILIO üè°\\r\\r\\r\\r\\n#quedateencasa\\r\\r\\...   \n",
       "...                                                 ...   \n",
       "9643  #water #foryou #followforfollowback #photograp...   \n",
       "9644  #like4likes #20likes #tagforlikes #instalikes ...   \n",
       "9645  üôàü•∞üòçüëâüèΩ @love_serie_karma #daancorona #daancoron...   \n",
       "9646  üí•üö® ùóôùóîùóüùóü ùóúùó¶ ùóñùó¢ùó†ùóúùó°ùóö üö®üí•‚Å£\\r\\r\\r\\r\\r\\n‚Å£\\r\\r\\r\\r\\r\\n...   \n",
       "9647  Follow me @kestine_kylie \\r\\r\\r\\r\\n.\\r\\r\\r\\r\\n...   \n",
       "\n",
       "                                           url  Q1_pertain_to_covid  \\\n",
       "0     https://www.instagram.com/p/B-A5FIIIEKJ/                    1   \n",
       "1     https://www.instagram.com/p/B-A74YQn_4a/                    1   \n",
       "2     https://www.instagram.com/p/B-AajnDp6GQ/                    2   \n",
       "3     https://www.instagram.com/p/B-AarR4pUQA/                    2   \n",
       "4     https://www.instagram.com/p/B-AE1y3HD-Z/                    1   \n",
       "...                                        ...                  ...   \n",
       "9643  https://www.instagram.com/p/CEZyheBgtMd/                    2   \n",
       "9644  https://www.instagram.com/p/CEZYoyRluKG/                    2   \n",
       "9645  https://www.instagram.com/p/CEZz-solaUF/                    2   \n",
       "9646  https://www.instagram.com/p/CEZZ4P4j3rh/                    2   \n",
       "9647  https://www.instagram.com/p/CEZZyqshFLq/                    2   \n",
       "\n",
       "      Q2_cetegory  Q2A_Type of Human  ...  Q8_threat_covid  \\\n",
       "0               1                  1  ...                1   \n",
       "1               1                  1  ...                1   \n",
       "2              99                 99  ...               99   \n",
       "3              99                 99  ...               99   \n",
       "4               1                  1  ...                1   \n",
       "...           ...                ...  ...              ...   \n",
       "9643           99                 99  ...               99   \n",
       "9644           99                 99  ...               99   \n",
       "9645           99                 99  ...               99   \n",
       "9646           99                 99  ...               99   \n",
       "9647           99                 99  ...               99   \n",
       "\n",
       "      Q9_susceptibility_covid  Q9A_Asian responsible for the covid  \\\n",
       "0                           1                                    2   \n",
       "1                           1                                    2   \n",
       "2                          99                                   99   \n",
       "3                          99                                   99   \n",
       "4                           1                                    2   \n",
       "...                       ...                                  ...   \n",
       "9643                       99                                   99   \n",
       "9644                       99                                   99   \n",
       "9645                       99                                   99   \n",
       "9646                       99                                   99   \n",
       "9647                       99                                   99   \n",
       "\n",
       "      Q10_solution_present  Q11_recommended_solution  misinformation  \\\n",
       "0                        1                         1               0   \n",
       "1                        2                        99               0   \n",
       "2                       99                        99               0   \n",
       "3                       99                        99               0   \n",
       "4                        1                         1               0   \n",
       "...                    ...                       ...             ...   \n",
       "9643                    99                        99               0   \n",
       "9644                    99                        99               0   \n",
       "9645                    99                        99               0   \n",
       "9646                    99                        99               0   \n",
       "9647                    99                        99               0   \n",
       "\n",
       "      Q12 Presence of conspiracy theory  Q12-Others  \\\n",
       "0                                     2          99   \n",
       "1                                     2          99   \n",
       "2                                    99          99   \n",
       "3                                    99          99   \n",
       "4                                     2          99   \n",
       "...                                 ...         ...   \n",
       "9643                                 99          99   \n",
       "9644                                 99          99   \n",
       "9645                                 99          99   \n",
       "9646                                 99          99   \n",
       "9647                                 99          99   \n",
       "\n",
       "      Q13.  Image of plague doctor costume             Note  \n",
       "0                                      2.0        Stay home  \n",
       "1                                      2.0              NaN  \n",
       "2                                     99.0              NaN  \n",
       "3                                     99.0              NaN  \n",
       "4                                      2.0  mask; Stay home  \n",
       "...                                    ...              ...  \n",
       "9643                                  99.0              NaN  \n",
       "9644                                  99.0              NaN  \n",
       "9645                                  99.0              NaN  \n",
       "9646                                  99.0              NaN  \n",
       "9647                                  99.0              NaN  \n",
       "\n",
       "[9589 rows x 26 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "ins_df = pd.read_csv('data/instagram_data.csv')\n",
    "ins_df = ins_df[ins_df['Contents'].notna()] # We might want to do something different here - SN\n",
    "ins_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>never #make a decision when you are .. #angry ...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>683</th>\n",
       "      <td>@JoeGoodmanJr. A solution. But the assumptions...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>698</th>\n",
       "      <td>Meanwhile, the so-called M√¢‚Ç¨‚Ñ¢s fans sit there ...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>What a great training course, lots of photos, ...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>@KarenL109 replace the carpet!! #shocking !!!!</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>733</th>\n",
       "      <td>@ChrisWarcraft The concept that a gay magazine...</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1042</th>\n",
       "      <td>81' Goal scorer Vidar Kjartansson comes off in...</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>689</th>\n",
       "      <td>Back to forest drama -\\nfrom my #Forest #music...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571</th>\n",
       "      <td>@alyssasimpson21 I gave up KENDRICK LAMAR to e...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>@stackee Charlie attempted to suffocate me wit...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3613 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweet category\n",
       "356   never #make a decision when you are .. #angry ...    anger\n",
       "683   @JoeGoodmanJr. A solution. But the assumptions...    anger\n",
       "698   Meanwhile, the so-called M√¢‚Ç¨‚Ñ¢s fans sit there ...      joy\n",
       "56    What a great training course, lots of photos, ...      joy\n",
       "415      @KarenL109 replace the carpet!! #shocking !!!!     fear\n",
       "...                                                 ...      ...\n",
       "733   @ChrisWarcraft The concept that a gay magazine...     fear\n",
       "1042  81' Goal scorer Vidar Kjartansson comes off in...     fear\n",
       "689   Back to forest drama -\\nfrom my #Forest #music...  sadness\n",
       "571   @alyssasimpson21 I gave up KENDRICK LAMAR to e...    anger\n",
       "119   @stackee Charlie attempted to suffocate me wit...      joy\n",
       "\n",
       "[3613 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "anger_df = pd.read_csv('data/twitter/anger.tsv', sep='\\t').drop(columns=['index', 'intensity'])\n",
    "fear_df = pd.read_csv('data/twitter/fear.tsv', sep='\\t').drop(columns=['index', 'intensity'])\n",
    "joy_df = pd.read_csv('data/twitter/joy.tsv', sep='\\t').drop(columns=['index', 'intensity'])\n",
    "sadness_df = pd.read_csv('data/twitter/sadness.tsv', sep='\\t').drop(columns=['index', 'intensity'])\n",
    "\n",
    "emotion_df = pd.concat([anger_df, fear_df, joy_df, sadness_df])\n",
    "emotion_df = shuffle(emotion_df)\n",
    "\n",
    "emotion_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: transformers in c:\\users\\hp\\anaconda3\\lib\\site-packages (4.12.5)\n",
      "Requirement already up-to-date: datasets in c:\\users\\hp\\anaconda3\\lib\\site-packages (1.16.1)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (1.21.4)\n",
      "Requirement already satisfied, skipping upgrade: tqdm>=4.27 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (4.50.2)\n",
      "Requirement already satisfied, skipping upgrade: packaging>=20.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (20.4)\n",
      "Requirement already satisfied, skipping upgrade: filelock in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied, skipping upgrade: regex!=2019.12.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (2020.10.15)\n",
      "Requirement already satisfied, skipping upgrade: requests in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (2.26.0)\n",
      "Requirement already satisfied, skipping upgrade: pyyaml>=5.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (5.3.1)\n",
      "Requirement already satisfied, skipping upgrade: tokenizers<0.11,>=0.10.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (0.10.3)\n",
      "Requirement already satisfied, skipping upgrade: huggingface-hub<1.0,>=0.1.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (0.1.2)\n",
      "Requirement already satisfied, skipping upgrade: sacremoses in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (0.0.46)\n",
      "Requirement already satisfied, skipping upgrade: fsspec[http]>=2021.05.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from datasets) (2021.11.0)\n",
      "Requirement already satisfied, skipping upgrade: xxhash in c:\\users\\hp\\anaconda3\\lib\\site-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied, skipping upgrade: multiprocess in c:\\users\\hp\\anaconda3\\lib\\site-packages (from datasets) (0.70.12.2)\n",
      "Requirement already satisfied, skipping upgrade: dill in c:\\users\\hp\\anaconda3\\lib\\site-packages (from datasets) (0.3.4)\n",
      "Requirement already satisfied, skipping upgrade: pandas in c:\\users\\hp\\anaconda3\\lib\\site-packages (from datasets) (1.1.3)\n",
      "Requirement already satisfied, skipping upgrade: aiohttp in c:\\users\\hp\\anaconda3\\lib\\site-packages (from datasets) (3.7.4)\n",
      "Requirement already satisfied, skipping upgrade: pyarrow!=4.0.0,>=3.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from datasets) (6.0.0)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied, skipping upgrade: six in c:\\users\\hp\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (1.16.0)\n",
      "Requirement already satisfied, skipping upgrade: idna<4,>=2.5; python_version >= \"3\" in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied, skipping upgrade: charset-normalizer~=2.0.0; python_version >= \"3\" in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied, skipping upgrade: urllib3<1.27,>=1.21.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.7)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied, skipping upgrade: typing-extensions>=3.7.4.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.0.0)\n",
      "Requirement already satisfied, skipping upgrade: click in c:\\users\\hp\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied, skipping upgrade: joblib in c:\\users\\hp\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (0.17.0)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.7.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.8.1)\n",
      "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pandas->datasets) (2020.1)\n",
      "Requirement already satisfied, skipping upgrade: attrs>=17.3.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (20.3.0)\n",
      "Requirement already satisfied, skipping upgrade: chardet<5.0,>=2.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: multidict<7.0,>=4.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (5.1.0)\n",
      "Requirement already satisfied, skipping upgrade: yarl<2.0,>=1.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.6.3)\n",
      "Requirement already satisfied, skipping upgrade: async-timeout<4.0,>=3.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (3.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "# MODEL = \"cardiffnlp/twitter-roberta-base-sentiment\" \n",
    "MODEL = \"roberta-base\" # Will likely change\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "# Note: How we preprocess may depend on model we use to transfer. \n",
    "# This comes from https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment\n",
    "def preprocess(text):\n",
    "    new_text = []\n",
    "    for t in text.split(\" \"):\n",
    "        t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
    "        t = 'http' if t.startswith('http') else t\n",
    "        new_text.append(t)\n",
    "    return \" \".join(new_text)\n",
    "\n",
    "# Load data into numpy arrays\n",
    "X = np.array(emotion_df['tweet'])\n",
    "Y = np.array(emotion_df['category'])\n",
    "Y_ints = np.array(pd.factorize(emotion_df['category'])[0])\n",
    "X_ins = np.array(ins_df['Contents'])\n",
    "east_asian = np.array(ins_df['Q5A.  If yes to Q5, what type of Asian'] == 1, dtype=int)\n",
    "\n",
    "# Preprocess text\n",
    "for i in range(len(X)):     X[i] = preprocess(X[i])\n",
    "for i in range(len(X_ins)): X_ins[i] = preprocess(X_ins[i])\n",
    "\n",
    "# Split into train/val/test sets\n",
    "TRAIN_PCT, VAL_PCT, TEST_PCT  = 0.6, 0.2, 0.2\n",
    "train_idx = int(TRAIN_PCT * len(X))\n",
    "val_idx = train_idx + int(VAL_PCT * len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = X[:train_idx], Y_ints[:train_idx]\n",
    "X_val, Y_val = X[train_idx:val_idx], Y_ints[train_idx:val_idx]\n",
    "X_test, Y_test = X[val_idx:], Y_ints[val_idx:]\n",
    "\n",
    "# Tokenize the data\n",
    "X_train_enc = tokenizer(list(X_train), return_tensors='pt', padding=True, truncation=True)\n",
    "X_val_enc = tokenizer(list(X_val), return_tensors='pt', padding=True, truncation=True)\n",
    "X_test_enc = tokenizer(list(X_test), return_tensors='pt', padding=True, truncation=True)\n",
    "X_ins_enc = tokenizer(list(X_ins), return_tensors='pt', padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: define our machine learning model, from our discussion it we can try deep learning models\n",
    "\n",
    "import os\n",
    "from torch.utils.data import (\n",
    "    Dataset, \n",
    "    DataLoader, \n",
    "    RandomSampler, \n",
    "    SequentialSampler\n",
    ")\n",
    "\n",
    "import math \n",
    "from transformers import  (\n",
    "    BertPreTrainedModel, \n",
    "    RobertaConfig, \n",
    "    RobertaTokenizerFast\n",
    ")\n",
    "\n",
    "from transformers.optimization import (\n",
    "    AdamW, \n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "\n",
    "from scipy.special import softmax\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    matthews_corrcoef,\n",
    "    roc_curve,\n",
    "    auc,\n",
    "    average_precision_score,\n",
    "    accuracy_score\n",
    ")\n",
    "\n",
    "from transformers.models.roberta.modeling_roberta import (\n",
    "    RobertaClassificationHead,\n",
    "    RobertaConfig,\n",
    "    RobertaModel,\n",
    ")\n",
    "\n",
    "num_labels = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs:  1\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():       \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('Number of GPUs: ',torch.cuda.device_count())\n",
    "else:\n",
    "    print('No GPU, using CPU.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 128 \n",
    "train_batch_size = 8\n",
    "test_batch_size = 8\n",
    "warmup_ratio = 0.06\n",
    "weight_decay=0.001\n",
    "gradient_accumulation_steps = 1\n",
    "num_train_epochs = 5 \n",
    "learning_rate = 1e-05\n",
    "adam_epsilon = 1e-08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaClassification: ['lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model=\n",
      " RobertaClassification(\n",
      "  (roberta): RobertaModel(\n",
      "    (embeddings): RobertaEmbeddings(\n",
      "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
      "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "      (token_type_embeddings): Embedding(1, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): RobertaEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): RobertaPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (classifier): RobertaClassificationHead(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (out_proj): Linear(in_features=768, out_features=4, bias=True)\n",
      "  )\n",
      ") \n",
      "\n"
     ]
    }
   ],
   "source": [
    "class RobertaClassification(BertPreTrainedModel):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super(RobertaClassification, self).__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.roberta = RobertaModel(config)\n",
    "        self.classifier = RobertaClassificationHead(config)\n",
    "        \n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, labels):\n",
    "        outputs = self.roberta(input_ids,attention_mask=attention_mask)\n",
    "        sequence_output = outputs[0]\n",
    "        logits = self.classifier(sequence_output)\n",
    "\n",
    "        outputs = (logits,) + outputs[2:]\n",
    "        \n",
    "        loss_fct = CrossEntropyLoss()\n",
    "        loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "        outputs = (loss,) + outputs\n",
    "\n",
    "        return outputs  # (loss), logits, (hidden_states), (attentions)\n",
    "\n",
    "config_class = RobertaConfig\n",
    "model_class = RobertaClassification\n",
    "\n",
    "config = config_class.from_pretrained(MODEL, num_labels=num_labels)\n",
    "model = model_class.from_pretrained(MODEL, config=config)\n",
    "print('Model=\\n',model,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-946b6e0e6405>:8: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  self.labels = torch.as_tensor(labels, dtype=torch.long)\n"
     ]
    }
   ],
   "source": [
    "class MyClassificationDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data,y):\n",
    "        text = data\n",
    "        labels=y\n",
    "        self.examples = text\n",
    "#         targets = tr.transform(labels)\n",
    "        self.labels = torch.as_tensor(labels, dtype=torch.long)\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples[\"input_ids\"])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return {key: self.examples[key][index] for key in self.examples}, self.labels[index]\n",
    "\n",
    "#X_train, Y_train = X[:train_idx], Y[:train_idx]\n",
    "#X_val, Y_val = X[train_idx:val_idx], Y[train_idx:val_idx]\n",
    "#X_test, Y_test = X[val_idx:], Y[val_idx:]\n",
    "\n",
    "train_dataset = MyClassificationDataset(X_train_enc,Y_train)\n",
    "val_dataset = MyClassificationDataset(X_val_enc, Y_val)\n",
    "test_dataset = MyClassificationDataset(X_test_enc, Y_test)\n",
    "ins_dataset = MyClassificationDataset(X_ins_enc, [-1.] * len(X_ins))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    0, 17762,    11,  ...,     1,     1,     1],\n",
      "        [    0,  1039, 12105,  ...,     1,     1,     1],\n",
      "        [    0, 48641,  2600,  ...,     1,     1,     1],\n",
      "        ...,\n",
      "        [    0,  4763,  9460,  ...,     1,     1,     1],\n",
      "        [    0,   642, 11867,  ...,     1,     1,     1],\n",
      "        [    0,  2527,   685,  ...,     1,     1,     1]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0'), 'labels': tensor([0, 0, 0, 2, 3, 3, 3, 3], device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "train_batch_size = 8\n",
    "val_batch_size = 8\n",
    "test_batch_size = 8\n",
    "\n",
    "def get_inputs_dict(batch):\n",
    "    inputs = {key: value.squeeze(1).to(device) for key, value in batch[0].items()}\n",
    "    inputs[\"labels\"] = batch[1].to(device)\n",
    "    return inputs\n",
    "\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "train_dataloader = DataLoader(train_dataset,sampler=train_sampler,batch_size=train_batch_size)\n",
    "\n",
    "val_sampler = SequentialSampler(val_dataset)\n",
    "val_dataloader = DataLoader(val_dataset, sampler=val_sampler, batch_size=val_batch_size)\n",
    "\n",
    "test_sampler = SequentialSampler(test_dataset)\n",
    "test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=test_batch_size)\n",
    "\n",
    "ins_sampler = SequentialSampler(ins_dataset)\n",
    "ins_dataloader = DataLoader(ins_dataset, sampler=ins_sampler, batch_size=test_batch_size)\n",
    "\n",
    "#Extract a batch as sanity-check\n",
    "batch = get_inputs_dict(next(iter(train_dataloader)))\n",
    "input_ids = batch['input_ids'].to(device)\n",
    "attention_mask = batch['attention_mask'].to(device)\n",
    "labels = batch['labels'].to(device)\n",
    "\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_total = len(train_dataloader) // gradient_accumulation_steps * num_train_epochs\n",
    "optimizer_grouped_parameters = []\n",
    "custom_parameter_names = set()\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "optimizer_grouped_parameters.extend(\n",
    "    [\n",
    "        {\n",
    "            \"params\": [\n",
    "                p\n",
    "                for n, p in model.named_parameters()\n",
    "                if n not in custom_parameter_names and not any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            \"weight_decay\": weight_decay,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [\n",
    "                p\n",
    "                for n, p in model.named_parameters()\n",
    "                if n not in custom_parameter_names and any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]\n",
    ")\n",
    "\n",
    "warmup_steps = math.ceil(t_total * warmup_ratio)\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate, eps=adam_epsilon)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=t_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: train our model using the loaded data\n",
    "model.to(device)\n",
    "\n",
    "model.zero_grad()\n",
    "\n",
    "def log_metrics(y, y_preds):\n",
    "    print(classification_report(y, y_preds, target_names=['Joy', 'Fear', 'Sadness', 'Anger']))\n",
    "    \n",
    "\n",
    "def train_epochs(num_train_epochs):\n",
    "    avg_loss=[]\n",
    "    avg_val_loss=[]\n",
    "    for epoch in range(num_train_epochs):\n",
    "\n",
    "        model.train()\n",
    "        epoch_loss = []\n",
    "    \n",
    "        for batch in train_dataloader:\n",
    "            batch = get_inputs_dict(batch)\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs[0]\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            model.zero_grad()\n",
    "            epoch_loss.append(loss.item())\n",
    "        \n",
    "        #evaluate model with test_df at the end of the epoch.\n",
    "        eval_loss = 0.0\n",
    "        nb_eval_steps = 0\n",
    "        n_batches = len(val_dataloader)\n",
    "        preds = np.empty((len(val_dataset), num_labels))\n",
    "        out_label_ids = np.empty((len(val_dataset)))\n",
    "        model.eval()\n",
    "    \n",
    "        for i,test_batch in enumerate(val_dataloader):\n",
    "            with torch.no_grad():\n",
    "                test_batch = get_inputs_dict(test_batch)\n",
    "                input_ids = test_batch['input_ids'].to(device)\n",
    "                attention_mask = test_batch['attention_mask'].to(device)\n",
    "                labels = test_batch['labels'].to(device)\n",
    "                outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                tmp_eval_loss, logits = outputs[:2]\n",
    "                eval_loss += tmp_eval_loss.item()\n",
    "            \n",
    "            nb_eval_steps += 1\n",
    "            start_index = test_batch_size * i\n",
    "            end_index = start_index + test_batch_size if i != (n_batches - 1) else len(test_dataset)\n",
    "            preds[start_index:end_index] = logits.detach().cpu().numpy()\n",
    "            out_label_ids[start_index:end_index] = test_batch[\"labels\"].detach().cpu().numpy()\n",
    "        \n",
    "        eval_loss = eval_loss / nb_eval_steps\n",
    "        model_outputs = preds\n",
    "        preds = np.argmax(preds, axis=1)\n",
    "        #result, wrong = compute_metrics(preds, model_outputs, out_label_ids)\n",
    "        epoch_loss=np.mean(epoch_loss)\n",
    "        print('epoch',epoch,'Training avg loss',epoch_loss)\n",
    "        print('epoch',epoch,'Testing  avg loss',eval_loss)\n",
    "        print('---------------------------------------------------\\n')\n",
    "        avg_loss.append(epoch_loss)\n",
    "        avg_val_loss.append(eval_loss)\n",
    "    report=log_metrics(Y_val, preds)\n",
    "    print(report)\n",
    "    avg_loss=np.mean(avg_loss)\n",
    "    avg_val_loss=np.mean(avg_val_loss)\n",
    "    accuracy=accuracy_score(Y_val, preds)\n",
    "    return avg_loss,avg_val_loss,report,accuracy\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: evaluate using accuracy to see how well our model is performing on the test set\n",
    "# Report the approach(es) you take for each of this task and your multi-class accuracy and \n",
    "#   per-class precision and recall for each emotion class (fear, anger, joy, and sadness) on the development set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():   \n",
    "    model.to(device)\n",
    "    eval_loss = 0.0\n",
    "    nb_eval_steps = 0\n",
    "    n_batches = len(test_dataloader)\n",
    "    preds = np.empty((len(test_dataset), num_labels))\n",
    "    out_label_ids = np.empty((len(test_dataset)))\n",
    "    model.eval()\n",
    "    for i,test_batch in enumerate(test_dataloader):\n",
    "        with torch.no_grad():\n",
    "            test_batch = get_inputs_dict(test_batch)\n",
    "            input_ids = test_batch['input_ids'].to(device)\n",
    "            attention_mask = test_batch['attention_mask'].to(device)\n",
    "            labels = test_batch['labels'].to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            tmp_eval_loss, logits = outputs[:2]\n",
    "            eval_loss += tmp_eval_loss.item()\n",
    "\n",
    "        nb_eval_steps += 1\n",
    "        start_index = test_batch_size * i\n",
    "        end_index = start_index + test_batch_size if i != (n_batches - 1) else len(test_dataset)\n",
    "        preds[start_index:end_index] = logits.detach().cpu().numpy()\n",
    "        out_label_ids[start_index:end_index] = test_batch[\"labels\"].detach().cpu().numpy()\n",
    "\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    model_outputs = preds\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "    print(\"classification report for test set\")\n",
    "    print(log_metrics(Y_test, preds))\n",
    "    accuracy=accuracy_score(Y_test, preds)\n",
    "    return eval_loss,accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train with epochs= 2\n",
      "epoch 0 Training avg loss 1.2790201891392359\n",
      "epoch 0 Testing  avg loss 0.7979225761942811\n",
      "---------------------------------------------------\n",
      "\n",
      "epoch 1 Training avg loss 0.6456484510339934\n",
      "epoch 1 Testing  avg loss 0.549698912344136\n",
      "---------------------------------------------------\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Joy       0.72      0.84      0.77       152\n",
      "        Fear       0.91      0.89      0.90       175\n",
      "     Sadness       0.79      0.84      0.81       237\n",
      "       Anger       0.83      0.66      0.73       158\n",
      "\n",
      "    accuracy                           0.81       722\n",
      "   macro avg       0.81      0.80      0.80       722\n",
      "weighted avg       0.81      0.81      0.81       722\n",
      "\n",
      "None\n",
      "classification report for test set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Joy       0.71      0.85      0.77       162\n",
      "        Fear       0.93      0.90      0.92       168\n",
      "     Sadness       0.78      0.80      0.79       235\n",
      "       Anger       0.86      0.69      0.77       159\n",
      "\n",
      "    accuracy                           0.81       724\n",
      "   macro avg       0.82      0.81      0.81       724\n",
      "weighted avg       0.82      0.81      0.81       724\n",
      "\n",
      "None\n",
      "train with epochs= 4\n",
      "epoch 0 Training avg loss 0.3613553149178899\n",
      "epoch 0 Testing  avg loss 0.5297074080496044\n",
      "---------------------------------------------------\n",
      "\n",
      "epoch 1 Training avg loss 0.2637409768256314\n",
      "epoch 1 Testing  avg loss 0.5329849216677658\n",
      "---------------------------------------------------\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Joy       0.84      0.80      0.82       152\n",
      "        Fear       0.89      0.90      0.90       175\n",
      "     Sadness       0.81      0.80      0.81       237\n",
      "       Anger       0.75      0.80      0.77       158\n",
      "\n",
      "    accuracy                           0.82       722\n",
      "   macro avg       0.82      0.82      0.82       722\n",
      "weighted avg       0.82      0.82      0.82       722\n",
      "\n",
      "None\n",
      "classification report for test set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Joy       0.80      0.77      0.78       162\n",
      "        Fear       0.92      0.92      0.92       168\n",
      "     Sadness       0.81      0.80      0.80       235\n",
      "       Anger       0.80      0.86      0.83       159\n",
      "\n",
      "    accuracy                           0.83       724\n",
      "   macro avg       0.83      0.83      0.83       724\n",
      "weighted avg       0.83      0.83      0.83       724\n",
      "\n",
      "None\n",
      "train with epochs= 6\n",
      "epoch 0 Training avg loss 0.19446766804010227\n",
      "epoch 0 Testing  avg loss 0.5510397660597176\n",
      "---------------------------------------------------\n",
      "\n",
      "epoch 1 Training avg loss 0.18123586081325788\n",
      "epoch 1 Testing  avg loss 0.5510397660597176\n",
      "---------------------------------------------------\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Joy       0.86      0.78      0.82       152\n",
      "        Fear       0.90      0.90      0.90       175\n",
      "     Sadness       0.80      0.82      0.81       237\n",
      "       Anger       0.76      0.78      0.77       158\n",
      "\n",
      "    accuracy                           0.82       722\n",
      "   macro avg       0.83      0.82      0.82       722\n",
      "weighted avg       0.83      0.82      0.82       722\n",
      "\n",
      "None\n",
      "classification report for test set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Joy       0.82      0.76      0.79       162\n",
      "        Fear       0.93      0.92      0.93       168\n",
      "     Sadness       0.80      0.82      0.81       235\n",
      "       Anger       0.81      0.85      0.83       159\n",
      "\n",
      "    accuracy                           0.84       724\n",
      "   macro avg       0.84      0.84      0.84       724\n",
      "weighted avg       0.84      0.84      0.84       724\n",
      "\n",
      "None\n",
      "train with epochs= 8\n",
      "epoch 0 Training avg loss 0.17325367650697593\n",
      "epoch 0 Testing  avg loss 0.5510397660597176\n",
      "---------------------------------------------------\n",
      "\n",
      "epoch 1 Training avg loss 0.17663556775181277\n",
      "epoch 1 Testing  avg loss 0.5510397660597176\n",
      "---------------------------------------------------\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Joy       0.86      0.78      0.82       152\n",
      "        Fear       0.90      0.90      0.90       175\n",
      "     Sadness       0.80      0.82      0.81       237\n",
      "       Anger       0.76      0.78      0.77       158\n",
      "\n",
      "    accuracy                           0.82       722\n",
      "   macro avg       0.83      0.82      0.82       722\n",
      "weighted avg       0.83      0.82      0.82       722\n",
      "\n",
      "None\n",
      "classification report for test set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Joy       0.82      0.76      0.79       162\n",
      "        Fear       0.93      0.92      0.93       168\n",
      "     Sadness       0.80      0.82      0.81       235\n",
      "       Anger       0.81      0.85      0.83       159\n",
      "\n",
      "    accuracy                           0.84       724\n",
      "   macro avg       0.84      0.84      0.84       724\n",
      "weighted avg       0.84      0.84      0.84       724\n",
      "\n",
      "None\n",
      "train with epochs= 10\n",
      "epoch 0 Training avg loss 0.17467107651995778\n",
      "epoch 0 Testing  avg loss 0.5510397660597176\n",
      "---------------------------------------------------\n",
      "\n",
      "epoch 1 Training avg loss 0.17572985365436966\n",
      "epoch 1 Testing  avg loss 0.5510397660597176\n",
      "---------------------------------------------------\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Joy       0.86      0.78      0.82       152\n",
      "        Fear       0.90      0.90      0.90       175\n",
      "     Sadness       0.80      0.82      0.81       237\n",
      "       Anger       0.76      0.78      0.77       158\n",
      "\n",
      "    accuracy                           0.82       722\n",
      "   macro avg       0.83      0.82      0.82       722\n",
      "weighted avg       0.83      0.82      0.82       722\n",
      "\n",
      "None\n",
      "classification report for test set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Joy       0.82      0.76      0.79       162\n",
      "        Fear       0.93      0.92      0.93       168\n",
      "     Sadness       0.80      0.82      0.81       235\n",
      "       Anger       0.81      0.85      0.83       159\n",
      "\n",
      "    accuracy                           0.84       724\n",
      "   macro avg       0.84      0.84      0.84       724\n",
      "weighted avg       0.84      0.84      0.84       724\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "train_loss=[]\n",
    "val_loss=[]\n",
    "val_acc=[]\n",
    "test_loss=[]\n",
    "test_acc=[]\n",
    "for epoch in range(2,12,2):\n",
    "    print(\"train with epochs=\",epoch)\n",
    "    avg_loss,avg_val_loss,report,accuracy=train_epochs(2)\n",
    "    train_loss.append(avg_loss)\n",
    "    val_loss.append(avg_val_loss)\n",
    "    val_acc.append(accuracy)\n",
    "    testloss,testacc=test()\n",
    "    test_loss.append(testloss)\n",
    "    test_acc.append(testacc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1ea16b27d00>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x=[2,4,6,8,10]\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title(\"Loss for twitter data\")\n",
    "plt.plot(x,train_loss,marker='o',label='train')\n",
    "plt.plot(x,val_loss,marker='o',label='validation')\n",
    "plt.plot(x,test_loss,marker='o',label='test')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1ea3ab2cac0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title(\"Accuracy for twitter data\")\n",
    "plt.plot(x,val_acc,marker='o',label='validation')\n",
    "plt.plot(x,test_acc,marker='o',label='test')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-2a8c9d109822>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mstart_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_batch_size\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mend_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstart_index\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtest_batch_size\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mn_batches\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[0mpreds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart_index\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mend_index\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0meval_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meval_loss\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mnb_eval_steps\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "# TODO: predict the submission data \n",
    "model.to(device)\n",
    "nb_eval_steps = 0\n",
    "n_batches = len(ins_dataloader)\n",
    "preds = np.empty((len(ins_dataset), num_labels))\n",
    "model.eval()\n",
    "    \n",
    "for i,test_batch in enumerate(ins_dataloader):\n",
    "    with torch.no_grad():\n",
    "        test_batch = get_inputs_dict(test_batch)\n",
    "        input_ids = test_batch['input_ids'].to(device)\n",
    "        attention_mask = test_batch['attention_mask'].to(device)\n",
    "        labels = test_batch['labels'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        _, logits = outputs[:2]\n",
    "\n",
    "    nb_eval_steps += 1\n",
    "    start_index = test_batch_size * i\n",
    "    end_index = start_index + test_batch_size if i != (n_batches - 1) else len(ins_dataset)\n",
    "    preds[start_index:end_index] = logits.detach().cpu().numpy()\n",
    "\n",
    "eval_loss = eval_loss / nb_eval_steps\n",
    "model_outputs = preds\n",
    "preds = np.argmax(preds, axis=1)\n",
    "\n",
    "np.savetxt('instagram_predictions.txt', preds) # We might want to do something different here - SN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "# from scipy.stats import spearmanr\n",
    "emotions = ['Joy', 'Fear', 'Sadness', 'Anger']\n",
    "\n",
    "for i in range(num_labels):\n",
    "    corr, _ = pearsonr(preds[:,i], east_asian)\n",
    "    print('Correlation with {}: {}'.format(emotions[i], corr))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
