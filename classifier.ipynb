{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "os.environ['OMP_NUM_THREADS'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "ins_df = pd.read_csv('data/instagram_data.csv')\n",
    "ins_df = ins_df[ins_df['Contents'].notna()] # We might want to do something different here - SN\n",
    "ins_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "anger_df = pd.read_csv('data/twitter/anger.tsv', sep='\\t').drop(columns=['index', 'intensity'])\n",
    "fear_df = pd.read_csv('data/twitter/fear.tsv', sep='\\t').drop(columns=['index', 'intensity'])\n",
    "joy_df = pd.read_csv('data/twitter/joy.tsv', sep='\\t').drop(columns=['index', 'intensity'])\n",
    "sadness_df = pd.read_csv('data/twitter/sadness.tsv', sep='\\t').drop(columns=['index', 'intensity'])\n",
    "\n",
    "emotion_df = pd.concat([anger_df, fear_df, joy_df, sadness_df])\n",
    "emotion_df = shuffle(emotion_df)\n",
    "\n",
    "emotion_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade transformers datasets emoji deep-translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from deep_translator import GoogleTranslator\n",
    "import emoji\n",
    "# TOKENIZER_MODEL = \"cardiffnlp/twitter-roberta-base-sentiment\" \n",
    "TOKENIZER_MODEL = \"digitalepidemiologylab/covid-twitter-bert-v2\"\n",
    "# TOKENIZER_MODEL = \"roberta-base\"\n",
    "# TOKENIZER_MODEL = \"gpt2\" # - not yet working - SN\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_MODEL)\n",
    "translator = GoogleTranslator(source='auto', target='en')\n",
    "\n",
    "# Note: How we preprocess may depend on model we use to transfer. \n",
    "# This comes from https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment\n",
    "def preprocess(text):\n",
    "    new_text = []\n",
    "    for t in text.split(\" \"):\n",
    "        t = '' if t.startswith('@') and len(t) > 1 else t\n",
    "        t = '' if t.startswith('http') else t\n",
    "        t = t.replace('\\r', '')\n",
    "        t = t.replace(\"\\n\", \" \") # Remove newlines\n",
    "        \n",
    "        # Remove hashtags but keep words\n",
    "        t = ' '.join(t.split(\"#\")) if '#' in t else t\n",
    "#         t = '' if len(t.split()) == 0 else t\n",
    "#         t = '' if len(t.split()) == 1 and t.split()[0] == '#' else t # remove empty \"#\"\n",
    "#         if len(t.split(\"#\")) > 1:\n",
    "#             t = ' #'.join(t.split(\"#\"))[1:] # separate hashtags\n",
    "            \n",
    "        # change emojis to be explanation of emoji\n",
    "        if emoji.get_emoji_regexp().search(t) != None:\n",
    "            t = ' '.join(emoji.demojize(i) for i in emoji.get_emoji_regexp().split(t))\n",
    "            t = t.replace(\"_\",\" \")\n",
    "            t = t.replace(\"-\",\" \")\n",
    "            t = t.replace(\":\",\" \")\n",
    "    #         t = emoji.get_emoji_regexp().sub(\"\", t)\n",
    "    \n",
    "        t = \" \".join(t.split()) # Remove excess whitespace\n",
    "        new_text.append(t)\n",
    "    \n",
    "    cleaned_text = \" \".join(new_text)\n",
    "#     try:\n",
    "#         cleaned_text = translator.translate(cleaned_text) # Translate non english to english\n",
    "#     except Exception as e:\n",
    "#         print(e)\n",
    "    \n",
    "    if len(cleaned_text.split()) == 0: return text # return original text if our cleaning made empty string\n",
    "    return cleaned_text\n",
    "\n",
    "# Load data into numpy arrays\n",
    "X = np.array(emotion_df['tweet'])\n",
    "Y = np.array(emotion_df['category'])\n",
    "Y_ints = np.array(pd.factorize(emotion_df['category'])[0])\n",
    "X_ins = np.array(ins_df['Contents'])\n",
    "east_asian = np.array(ins_df['Q5A.  If yes to Q5, what type of Asian'] == 1, dtype=int)\n",
    "\n",
    "# Preprocess text\n",
    "for i in range(len(X)):     X[i] = preprocess(X[i])\n",
    "for i in range(len(X_ins)): X_ins[i] = preprocess(X_ins[i])\n",
    "\n",
    "# Split into train/val/test sets\n",
    "TRAIN_PCT, VAL_PCT, TEST_PCT  = 0.6, 0.2, 0.2\n",
    "train_idx = int(TRAIN_PCT * len(X))\n",
    "val_idx = train_idx + int(VAL_PCT * len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(30): \n",
    "    print(X_ins[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = X[:train_idx], Y_ints[:train_idx]\n",
    "X_val, Y_val = X[train_idx:val_idx], Y_ints[train_idx:val_idx]\n",
    "X_test, Y_test = X[val_idx:], Y_ints[val_idx:]\n",
    "\n",
    "# Tokenize the data\n",
    "if TOKENIZER_MODEL == 'gpt2':\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "X_train_enc = tokenizer(list(X_train), return_tensors='pt', padding=True, truncation=True)\n",
    "X_val_enc = tokenizer(list(X_val), return_tensors='pt', padding=True, truncation=True)\n",
    "X_test_enc = tokenizer(list(X_test), return_tensors='pt', padding=True, truncation=True)\n",
    "X_ins_enc = tokenizer(list(X_ins), return_tensors='pt', padding=True, truncation=True, max_length=X_train_enc['input_ids'].shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: define our machine learning model, from our discussion it we can try deep learning models\n",
    "\n",
    "import os\n",
    "from torch.utils.data import (\n",
    "    Dataset, \n",
    "    DataLoader, \n",
    "    RandomSampler, \n",
    "    SequentialSampler\n",
    ")\n",
    "\n",
    "import math \n",
    "from transformers import  (\n",
    "    BertPreTrainedModel, \n",
    "    RobertaConfig, \n",
    "    RobertaTokenizerFast\n",
    ")\n",
    "\n",
    "from transformers.optimization import (\n",
    "    AdamW, \n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "\n",
    "from scipy.special import softmax\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    matthews_corrcoef,\n",
    "    roc_curve,\n",
    "    auc,\n",
    "    average_precision_score,\n",
    "    accuracy_score\n",
    ")\n",
    "\n",
    "from transformers.models.roberta.modeling_roberta import (\n",
    "    RobertaClassificationHead,\n",
    "    RobertaConfig,\n",
    "    RobertaModel,\n",
    ")\n",
    "\n",
    "from transformers import AutoModel\n",
    "from torch import nn\n",
    "\n",
    "num_labels = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():       \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('Number of GPUs: ',torch.cuda.device_count())\n",
    "else:\n",
    "    print('No GPU, using CPU.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 128 \n",
    "train_batch_size = 8\n",
    "test_batch_size = 8\n",
    "warmup_ratio = 0.06\n",
    "weight_decay=0.0\n",
    "gradient_accumulation_steps = 1\n",
    "num_train_epochs = 5 \n",
    "learning_rate = 1e-05\n",
    "adam_epsilon = 1e-08\n",
    "\n",
    "hidden_units = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobertaClassification(BertPreTrainedModel):\n",
    "    \n",
    "    def __init__(self, config, MODEL=None, num_labels=None, pretrained_output_size=None, hidden_units=None):\n",
    "        super(RobertaClassification, self).__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.roberta = RobertaModel(config)\n",
    "        self.classifier = RobertaClassificationHead(config)\n",
    "        \n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, labels):\n",
    "        outputs = self.roberta(input_ids,attention_mask=attention_mask)\n",
    "        sequence_output = outputs[0]\n",
    "        \n",
    "        logits = self.classifier(sequence_output)\n",
    "\n",
    "        outputs = (logits,) + outputs[2:]\n",
    "        \n",
    "        loss_fct = CrossEntropyLoss()\n",
    "        loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "        outputs = (loss,) + outputs\n",
    "\n",
    "        return outputs  # (loss), logits, (hidden_states), (attentions)\n",
    "\n",
    "config_class = RobertaConfig\n",
    "model_class = RobertaClassification\n",
    "\n",
    "config = config_class.from_pretrained(MODEL, num_labels=num_labels)\n",
    "model = model_class.from_pretrained(MODEL, config=config)\n",
    "print('Model=\\n',model,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to use a different model than roberta\n",
    "# Uncomment MODEL you want\n",
    "# MODEL, pretrained_output_size = \"gpt2\", 768 # - not yet working\n",
    "# MODEL, pretrained_output_size = \"roberta-base\", 768\n",
    "# MODEL, pretrained_output_size = \"cardiffnlp/twitter-roberta-base-sentiment\", 768 \n",
    "MODEL, pretrained_output_size = \"digitalepidemiologylab/covid-twitter-bert-v2\", 1024\n",
    "\n",
    "assert MODEL == TOKENIZER_MODEL\n",
    "\n",
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self, config, MODEL, num_labels, pretrained_output_size, hidden_units):\n",
    "        super(Model, self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.pretrained_model = AutoModel.from_pretrained(MODEL)\n",
    "#         self.drop1 = nn.Dropout(0.2)\n",
    "        \n",
    "        self.linear1 = nn.Linear(pretrained_output_size, hidden_units)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "#         self.drop2 = nn.Dropout(0.2)\n",
    "        self.linear2 = nn.Linear(hidden_units, self.num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels):\n",
    "        output = self.pretrained_model(input_ids, attention_mask=attention_mask)\n",
    "#         l1 = self.drop1(self.linear1(output.pooler_output))\n",
    "        l1 = self.linear1(output[0])\n",
    "        relu = self.relu(l1)\n",
    "        \n",
    "#         out = self.drop2(self.linear2(relu))\n",
    "        out = self.linear2(relu)\n",
    "        \n",
    "        loss_fct = CrossEntropyLoss()\n",
    "        loss = loss_fct(out.view(-1, self.num_labels), labels.view(-1))\n",
    "        \n",
    "        return loss, out\n",
    "\n",
    "model = Model(None, MODEL, num_labels, pretrained_output_size, hidden_units)\n",
    "print('Model=\\n',model,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyClassificationDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data,y):\n",
    "        text = data\n",
    "        labels=y\n",
    "        self.examples = text\n",
    "#         targets = tr.transform(labels)\n",
    "        self.labels = torch.as_tensor(labels, dtype=torch.long)\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples[\"input_ids\"])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return {key: self.examples[key][index] for key in self.examples}, self.labels[index]\n",
    "\n",
    "\n",
    "train_dataset = MyClassificationDataset(X_train_enc,Y_train)\n",
    "val_dataset = MyClassificationDataset(X_val_enc, Y_val)\n",
    "test_dataset = MyClassificationDataset(X_test_enc, Y_test)\n",
    "ins_dataset = MyClassificationDataset(X_ins_enc, [0.] * len(X_ins))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 8\n",
    "val_batch_size = 8\n",
    "test_batch_size = 8\n",
    "\n",
    "def get_inputs_dict(batch):\n",
    "    inputs = {key: value.squeeze(1).to(device) for key, value in batch[0].items()}\n",
    "    inputs[\"labels\"] = batch[1].to(device)\n",
    "    return inputs\n",
    "\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "train_dataloader = DataLoader(train_dataset,sampler=train_sampler,batch_size=train_batch_size)\n",
    "\n",
    "val_sampler = SequentialSampler(val_dataset)\n",
    "val_dataloader = DataLoader(val_dataset, sampler=val_sampler, batch_size=val_batch_size)\n",
    "\n",
    "test_sampler = SequentialSampler(test_dataset)\n",
    "test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=test_batch_size)\n",
    "\n",
    "ins_sampler = SequentialSampler(ins_dataset)\n",
    "ins_dataloader = DataLoader(ins_dataset, sampler=ins_sampler, batch_size=test_batch_size)\n",
    "\n",
    "#Extract a batch as sanity-check\n",
    "batch = get_inputs_dict(next(iter(train_dataloader)))\n",
    "input_ids = batch['input_ids'].to(device)\n",
    "attention_mask = batch['attention_mask'].to(device)\n",
    "labels = batch['labels'].to(device)\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_opts(model):\n",
    "    t_total = len(train_dataloader) // gradient_accumulation_steps * num_train_epochs\n",
    "    optimizer_grouped_parameters = []\n",
    "    custom_parameter_names = set()\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_grouped_parameters.extend(\n",
    "        [\n",
    "            {\n",
    "                \"params\": [\n",
    "                    p\n",
    "                    for n, p in model.named_parameters()\n",
    "                    if n not in custom_parameter_names and not any(nd in n for nd in no_decay)\n",
    "                ],\n",
    "                \"weight_decay\": weight_decay,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [\n",
    "                    p\n",
    "                    for n, p in model.named_parameters()\n",
    "                    if n not in custom_parameter_names and any(nd in n for nd in no_decay)\n",
    "                ],\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    warmup_steps = math.ceil(t_total * warmup_ratio)\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate, eps=adam_epsilon)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=t_total)\n",
    "    return optimizer, scheduler\n",
    "\n",
    "optimizer, scheduler = setup_opts(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: train our model using the loaded data\n",
    "model.to(device)\n",
    "\n",
    "model.zero_grad()\n",
    "\n",
    "def log_metrics(y, y_preds):\n",
    "    print(classification_report(y, y_preds, target_names=['Joy', 'Fear', 'Sadness', 'Anger']))\n",
    "    \n",
    "\n",
    "def train_epochs(num_train_epochs):\n",
    "    avg_loss=[]\n",
    "    avg_val_loss=[]\n",
    "    for epoch in range(num_train_epochs):\n",
    "\n",
    "        model.train()\n",
    "        epoch_loss = []\n",
    "    \n",
    "        for batch in train_dataloader:\n",
    "            batch = get_inputs_dict(batch)\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs[0]\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            model.zero_grad()\n",
    "            epoch_loss.append(loss.item())\n",
    "        \n",
    "        #evaluate model with test_df at the end of the epoch.\n",
    "        eval_loss = 0.0\n",
    "        nb_eval_steps = 0\n",
    "        n_batches = len(val_dataloader)\n",
    "        preds = np.empty((len(val_dataset), num_labels))\n",
    "        out_label_ids = np.empty((len(val_dataset)))\n",
    "        model.eval()\n",
    "    \n",
    "        for i,test_batch in enumerate(val_dataloader):\n",
    "            with torch.no_grad():\n",
    "                test_batch = get_inputs_dict(test_batch)\n",
    "                input_ids = test_batch['input_ids'].to(device)\n",
    "                attention_mask = test_batch['attention_mask'].to(device)\n",
    "                labels = test_batch['labels'].to(device)\n",
    "                outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                tmp_eval_loss, logits = outputs[:2]\n",
    "                eval_loss += tmp_eval_loss.item()\n",
    "            \n",
    "            nb_eval_steps += 1\n",
    "            start_index = test_batch_size * i\n",
    "            end_index = start_index + test_batch_size if i != (n_batches - 1) else len(test_dataset)\n",
    "            preds[start_index:end_index] = logits.detach().cpu().numpy()\n",
    "            out_label_ids[start_index:end_index] = test_batch[\"labels\"].detach().cpu().numpy()\n",
    "        \n",
    "        eval_loss = eval_loss / nb_eval_steps\n",
    "        model_outputs = preds\n",
    "        preds = np.argmax(preds, axis=1)\n",
    "        #result, wrong = compute_metrics(preds, model_outputs, out_label_ids)\n",
    "        epoch_loss=np.mean(epoch_loss)\n",
    "        print('epoch',epoch,'Training avg loss',epoch_loss)\n",
    "        print('epoch',epoch,'Testing  avg loss',eval_loss)\n",
    "        print('---------------------------------------------------\\n')\n",
    "        avg_loss.append(epoch_loss)\n",
    "        avg_val_loss.append(eval_loss)\n",
    "        \n",
    "    report=log_metrics(Y_val, preds)\n",
    "    print(report)\n",
    "    avg_loss=np.mean(avg_loss)\n",
    "    avg_val_loss=np.mean(avg_val_loss)\n",
    "    accuracy=accuracy_score(Y_val, preds)\n",
    "    return avg_loss,avg_val_loss,report,accuracy\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():   \n",
    "    model.to(device)\n",
    "    eval_loss = 0.0\n",
    "    nb_eval_steps = 0\n",
    "    n_batches = len(test_dataloader)\n",
    "    preds = np.empty((len(test_dataset), num_labels))\n",
    "    out_label_ids = np.empty((len(test_dataset)))\n",
    "    model.eval()\n",
    "    for i,test_batch in enumerate(test_dataloader):\n",
    "        with torch.no_grad():\n",
    "            test_batch = get_inputs_dict(test_batch)\n",
    "            input_ids = test_batch['input_ids'].to(device)\n",
    "            attention_mask = test_batch['attention_mask'].to(device)\n",
    "            labels = test_batch['labels'].to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            tmp_eval_loss, logits = outputs[:2]\n",
    "            eval_loss += tmp_eval_loss.item()\n",
    "\n",
    "        nb_eval_steps += 1\n",
    "        start_index = test_batch_size * i\n",
    "        end_index = start_index + test_batch_size if i != (n_batches - 1) else len(test_dataset)\n",
    "        preds[start_index:end_index] = logits.detach().cpu().numpy()\n",
    "        out_label_ids[start_index:end_index] = test_batch[\"labels\"].detach().cpu().numpy()\n",
    "\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    model_outputs = preds\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "    print(\"classification report for test set\")\n",
    "    print(log_metrics(Y_test, preds))\n",
    "    accuracy=accuracy_score(Y_test, preds)\n",
    "    return eval_loss,accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_loss=[]\n",
    "val_loss=[]\n",
    "val_acc=[]\n",
    "test_loss=[]\n",
    "test_acc=[]\n",
    "for epoch in range(2,12,2):\n",
    "    print(\"train with epochs=\",epoch)\n",
    "#     model = model_class.from_pretrained(MODEL, config=config)\n",
    "    model = Model(None, MODEL, num_labels, pretrained_output_size, hidden_units)\n",
    "    model.to(device)\n",
    "    optimizer, scheduler = setup_opts(model)\n",
    "    \n",
    "    avg_loss,avg_val_loss,report,accuracy=train_epochs(epoch)\n",
    "    \n",
    "    train_loss.append(avg_loss)\n",
    "    val_loss.append(avg_val_loss)\n",
    "    val_acc.append(accuracy)\n",
    "    testloss,testacc=test()\n",
    "    test_loss.append(testloss)\n",
    "    test_acc.append(testacc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x=[2,4,6,8,10]\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title(\"Loss for twitter data\")\n",
    "plt.plot(x,train_loss,marker='o',label='train')\n",
    "plt.plot(x,val_loss,marker='o',label='validation')\n",
    "plt.plot(x,test_loss,marker='o',label='test')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title(\"Accuracy for twitter data\")\n",
    "plt.plot(x,val_acc,marker='o',label='validation')\n",
    "plt.plot(x,test_acc,marker='o',label='test')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: predict the submission data \n",
    "nb_eval_steps = 0\n",
    "n_batches = len(ins_dataloader)\n",
    "preds = np.empty((len(ins_dataset), num_labels))\n",
    "model.eval()\n",
    "    \n",
    "for i,test_batch in enumerate(ins_dataloader):\n",
    "    with torch.no_grad():\n",
    "        test_batch = get_inputs_dict(test_batch)\n",
    "        input_ids = test_batch['input_ids'].to(device)\n",
    "        attention_mask = test_batch['attention_mask'].to(device)\n",
    "        labels = test_batch['labels'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        _, logits = outputs[:2]\n",
    "\n",
    "    nb_eval_steps += 1\n",
    "    start_index = test_batch_size * i\n",
    "    end_index = start_index + test_batch_size if i != (n_batches - 1) else len(ins_dataset)\n",
    "    preds[start_index:end_index] = logits.detach().cpu().numpy()\n",
    "\n",
    "model_outputs = preds\n",
    "preds = np.argmax(preds, axis=1)\n",
    "np.savetxt('instagram_predictions.txt', preds) # We might want to do something different here - SN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "# from scipy.stats import spearmanr\n",
    "emotions = ['Joy', 'Fear', 'Sadness', 'Anger']\n",
    "\n",
    "preds_one_hot = np.zeros((len(preds), preds.max()+1))\n",
    "preds_one_hot[np.arange(len(preds)),preds] = 1\n",
    "\n",
    "for i in range(num_labels):\n",
    "    corr, _ = pearsonr(preds_one_hot[:,i], east_asian)\n",
    "    print('Correlation with {}: {}'.format(emotions[i], corr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    print('Prediction: {} \\nProcessed:\\n{}\\nUnprocessed:\\n{}\\n\\n'.format(emotions[preds[i]], X_ins[i],np.array(ins_df['Contents'])[i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
